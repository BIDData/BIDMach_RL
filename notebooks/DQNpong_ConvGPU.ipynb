{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning\n",
    "\n",
    "In this notebook, you will implement a deep Q-Learning reinforcement algorithm. The implementation borrows ideas from both the original DeepMind Nature paper and the more recent asynchronous version:<br/>\n",
    "[1] \"Human-Level Control through Deep Reinforcement Learning\" by Mnih et al. 2015<br/>\n",
    "[2] \"Asynchronous Methods for Deep Reinforcement Learning\" by Mnih et al. 2016.<br/>\n",
    "\n",
    "In particular:\n",
    "* We use separate target and Q-functions estimators with periodic updates to the target estimator. \n",
    "* We use several concurrent \"threads\" rather than experience replay to generate less biased gradient updates. \n",
    "* Threads are actually synchronized so we start each one at a random number of moves.\n",
    "* We use an epsilon-greedy policy that blends random moves with policy moves.\n",
    "* We taper the random search parameter (epsilon) and the learning rate to zero during training.\n",
    "\n",
    "This gives a simple and reasonably fast general-purpose RL algorithm. We use it here for the Cartpole environment from OpenAI Gym, but it can easily be adapted to others. For this notebook, you will implement 4 steps:\n",
    "\n",
    "1. The backward step for the Q-estimator\n",
    "2. The $\\epsilon$-greedy policy\n",
    "3. \"asynchronous\" initialization \n",
    "4. The Q-learning algorithm\n",
    "\n",
    "To get started, we import some prerequisites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 CUDA devices found, CUDA version 8.0\n"
     ]
    }
   ],
   "source": [
    "import BIDMat.{CMat,CSMat,DMat,Dict,FMat,FND,FFilter,Filter,GFilter,GMat,GDMat,GIMat,GLMat,GSMat,GSDMat,GND,HMat,IDict,Image,IMat,LMat,Mat,SMat,SBMat,SDMat,TMat}\n",
    "import BIDMat.MatFunctions._\n",
    "import BIDMat.SciFunctions._\n",
    "import BIDMat.Solvers._\n",
    "import BIDMat.JPlotting._\n",
    "import BIDMach.Learner\n",
    "\n",
    "import BIDMach.models.{Click,FM,GLM,KMeans,KMeansw,LDA,LDAgibbs,Model,NMF,SFA,RandomForest,SVD}\n",
    "import BIDMach.networks.{Net}\n",
    "import BIDMach.datasources.{DataSource,MatSource,FileSource,SFileSource}\n",
    "import BIDMach.datasinks.{DataSink,MatSink}\n",
    "import BIDMach.mixins.{CosineSim,Perplexity,Top,L1Regularizer,L2Regularizer}\n",
    "import BIDMach.updaters.{ADAGrad,Batch,BatchNorm,Grad,IncMult,IncNorm,Telescoping}\n",
    "import BIDMach.causal.{IPTW}\n",
    "import BIDMach.rl.ALE\n",
    "\n",
    "Mat.checkMKL(false)\n",
    "Mat.checkCUDA\n",
    "Mat.setInline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "  0.050579   0.39953\n",
       "   0.73763   0.91040\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand(2,2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below lists some parameters you can tune. They should be self-explanatory. They are currently set to train CartPole-V0 to a \"solved\" score (> 195) most of the time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nsteps = 20000001                    // Number of steps to run (game actions per environment)\n",
    "val npar = 16                         // Number of parallel environments\n",
    "val target_window = 50                  // Interval to update target estimator from q-estimator\n",
    "val discount_factor = 0.99f              // Reward discount factor\n",
    "val printsteps = 10000                   // Number of steps between printouts\n",
    "val render = false                       // Whether to render an environment while training\n",
    "\n",
    "val epsilon_start = 0.5f                 // Parameters for epsilon-greedy policy: initial epsilon\n",
    "val epsilon_end = 0.1f                  // Final epsilon\n",
    "val neps = (0.9*nsteps).toInt            // Number of steps to decay epsilon\n",
    "\n",
    "//val learning_rate = 5e-4f\n",
    "val learning_rate = 3e-6f                // Initial learning rate\n",
    "val lr_end = learning_rate               // Final learning rate\n",
    "val nlr = neps                           // Steps to decay learning rate\n",
    "val gsq_decay = 0.99f                   // Decay factor for RMSProp\n",
    "val momentum_decay = 0.9f\n",
    "val gclip = 1f\n",
    "val rmseps = 3e-6f\n",
    "val rmsevery = 5\n",
    "\n",
    "val nhidden = 16                         // Number of hidden layers for estimators\n",
    "val nhidden2 = 32\n",
    "val nhidden3 = 256\n",
    "\n",
    "val init_moves = 10000                   // Upper bound on random number of moves to take initially\n",
    "val nwindow = 4                          // Sensing window = last n images in a state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are environment-specific parameters. The function \"preprocess\" should process an observation returned by the environment into a vector for training. For CartPole we simply append a 1 to implement bias in the first layer. \n",
    "\n",
    "For visual environments you would typically crop, downsample to 80x80, set color to a single bit (foreground/background), and flatten to a vector. \n",
    "\n",
    "*nfeats* is the dimension of the vector output by *preprocess*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "val game_bin=\"/code/ALE/roms/Pong.bin\"                 // Model type and action definitions\n",
    "val VALID_ACTIONS = irow(0, 3, 4)\n",
    "val nactions= VALID_ACTIONS.length\n",
    "val nfeats = 80*80  \n",
    "val height = 80\n",
    "val width = 80\n",
    "\n",
    "def preprocess(I:FND, out:FMat):FMat = {\n",
    "//  Preprocess Pong game frames into vectors.\n",
    "//  Input:\n",
    "//    - (3,160,210) uint8 frame representing Pong game screen.\n",
    "//  Returns:\n",
    "//    - Downsampled (DxD) matrix of 0s and 1s, \"raveled\" into a 1-D vector.\n",
    "    var i = 0;\n",
    "    val res = if (out.asInstanceOf[AnyRef] == null) zeros(80*80,1) else out;\n",
    "    while (i < 80) {\n",
    "        var j = 0;\n",
    "        while (j < 80) {\n",
    "            val x = I.data(3*(j*2 + 160 * (i*2 + 35)));\n",
    "            res.data(j + 80*i) = {if (x == 144f || x == 109f) 0f else {if (x != 0f) 1f else 0f}};\n",
    "            j += 1;\n",
    "        }\n",
    "        i += 1;\n",
    "    }\n",
    "    res\n",
    "}\n",
    "\n",
    "def preprocess(I:Array[Byte], out:FMat):FMat = {\n",
    "//  Preprocess Pong game frames into vectors.\n",
    "//  Input:\n",
    "//    - (3,160,210) uint8 frame representing Pong game screen.\n",
    "//  Returns:\n",
    "//    - Downsampled (DxD) matrix of 0s and 1s, \"raveled\" into a 1-D vector.\n",
    "    var i = 0;\n",
    "    val res = if (out.asInstanceOf[AnyRef] == null) zeros(80*80,1) else out;\n",
    "    while (i < 80) {\n",
    "        var j = 0;\n",
    "        while (j < 80) {\n",
    "            val x = I(j*2 + 160 * (i*2 + 35));\n",
    "            res.data(j + 80*i) = {if (x == 34) 0f else {if (x != 0) 1f else 0f}};\n",
    "            j += 1;\n",
    "        }\n",
    "        i += 1;\n",
    "    }\n",
    "    res\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the Q-estimator class. We use two instances of this class, one for the target estimator, and one for the Q-estimator. The Q function is normally represented as a scalar $Q(x,a)$ where $x$ is the state and $a$ is an action. For ease of implementation, we actually estimate a vector-valued function $Q(x,.)$ which returns the estimated reward for every action. The model here has just a single hidden layer:\n",
    "\n",
    "<pre>\n",
    "Input Layer (nfeats) => FC Layer => RELU => FC Layer => Output (naction values)\n",
    "</pre>\n",
    "\n",
    "## 1. Implement Q-estimator gradient\n",
    "Your first task is to implement the\n",
    "<pre>Estimator.gradient(s, a, y)</pre>\n",
    "method for this class. **gradient** should compute the gradients wrt weight arrays W1 and W2 into\n",
    "<pre>self.grad['W1']\n",
    "self.grad['W2']</pre>\n",
    "respectively. Both <code>a</code> and <code>y</code> are vectors. Be sure to update only the output layer weights corresponding to the given action vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import scala.collection.mutable.HashMap;\n",
    "import BIDMat.GFilter._\n",
    "import jcuda.jcudnn.cudnnTensorFormat._\n",
    "import jcuda.jcudnn.cudnnConvolutionFwdAlgo._\n",
    "\n",
    "// Best for debugging - matches the CPU tensor order:\n",
    "//val tformat = CUDNN_TENSOR_NHWC;\n",
    "//val talgo = CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM;\n",
    "\n",
    "// Best for performance:\n",
    "val tformat = CUDNN_TENSOR_NCHW;\n",
    "val talgo = CUDNN_CONVOLUTION_FWD_ALGO_GEMM;\n",
    "\n",
    "class Estimator(nhidden:Int, nhidden2:Int, nhidden3:Int, nactions:Int) {\n",
    "\n",
    "//        \"\"\" Create model matrices, and gradient and squared gradient buffers\"\"\"\n",
    "    val model = new HashMap[String, GFilter]();                 // Store model params in Filters, even for FC layers\n",
    "    val grad = new HashMap[String, GFilter]();\n",
    "    val gradsq = new HashMap[String, GFilter]();\n",
    "    \n",
    "    // Filter specifiers are (h, w, indepth, outdepth, stride, pad)\n",
    "    model(\"W1\") = GFilter2Ddn(7,7,nwindow,nhidden,4,3);                         // First conv layer\n",
    "    model(\"W2\") = GFilter2Ddn(3,3,nhidden,nhidden2,2,1);                        // Second conv layer\n",
    "    model(\"W3\") = GFilter2Ddn(10,10,nhidden2,nhidden3,1,0);                     // This is actually an FC layer\n",
    "    model(\"W4\") = GFilter2Ddn(1,1,nhidden3,nactions,1,0);                       // Second FC layer\n",
    "    \n",
    "    model(\"W1\")(?) = GMat(normrnd(0,1/math.sqrt(7*7*nwindow).toFloat,7*7*nwindow*nhidden,1))\n",
    "    model(\"W2\")(?) = GMat(normrnd(0,1/math.sqrt(3*3*nhidden).toFloat,3*3*nhidden*nhidden2,1))\n",
    "    model(\"W3\")(?) = GMat(normrnd(0,1/math.sqrt(10*10*nhidden2).toFloat,10*10*nhidden2*nhidden3,1))\n",
    "    model(\"W4\")(?) = GMat(normrnd(0,1/math.sqrt(nhidden3).toFloat,nhidden3*nactions,1))\n",
    "    \n",
    "//    normrnd(0,1/math.sqrt(7*7*nwindow).toFloat, model(\"W1\"))                    // \"Xavier\" initialization    \n",
    "//    normrnd(0,1/math.sqrt(3*3*nhidden).toFloat, model(\"W2\"))                    // which is normal random with variance 1/indegree\n",
    "//    normrnd(0,1/math.sqrt(10*10*nhidden2).toFloat, model(\"W3\"))\n",
    "//    normrnd(0,1/math.sqrt(nhidden3).toFloat, model(\"W4\"))   \n",
    "    \n",
    "    for ((k,v) <- model) {\n",
    "        grad.put(k, v.copy);\n",
    "        gradsq.put(k, v.copy);\n",
    "        grad(k).clear\n",
    "        gradsq(k).clear\n",
    "        v.tensorFormat = tformat\n",
    "        grad(k).tensorFormat = tformat\n",
    "        gradsq(k).tensorFormat = tformat\n",
    "        v.fwdAlgo = talgo\n",
    "        grad(k).fwdAlgo = talgo\n",
    "        gradsq(k).fwdAlgo = talgo\n",
    "    } \n",
    "\n",
    "    def _forward(s:GND):(GMat, GND, GND, GMat) = {\n",
    "//        \"\"\" Run the model forward given a state as input.\n",
    "//    returns action predictions and the hidden state\"\"\"     \n",
    "        val h = model(\"W1\") * s                  // Forward convolution, h is a tensor\n",
    "        h ~ h *@ (h>=0)                          // ReLU nonlinearity\n",
    "        val h2 = model(\"W2\") * h                 // Forward convolution, h2 is a tensor\n",
    "        h2 ~ h2 *@ (h2 >= 0);\n",
    "        val h3 = model(\"W3\").asMat ^* h2.asMat;  // FC matrix multiply, h3 is a matrix\n",
    "        h3 ~ h3 *@ (h3 >= 0);                   \n",
    "        val rew = model(\"W4\").asMat ^* h3;       // Final FC layer, rew is a matrix\n",
    "        (rew, h, h2, h3)\n",
    "    }\n",
    "    \n",
    "    def formatInput(s:FMat):GND = {\n",
    "        if (tformat == CUDNN_TENSOR_NCHW) {\n",
    "            GMat(s).asND(nwindow,height,width,npar);\n",
    "        } else {\n",
    "            GMat(s).asND(height,width,nwindow,npar).transpose(2,0,1,3);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def forward(s:FMat):(GMat, GND, GND, GMat) = {\n",
    "        val ss = formatInput(s);\n",
    "        _forward(ss);\n",
    "    }\n",
    "    \n",
    "    \n",
    "    def predict(s:FMat):FMat = {\n",
    "//        \"\"\" Predict the action rewards from a given input state\"\"\"\n",
    "        val (rew, h, h2, h3) = forward(s);\n",
    "        FMat(rew)\n",
    "    }\n",
    "    \n",
    "    val selector = irow(0->npar)*nactions;\n",
    "    val arew = zeros(nactions, npar);\n",
    "              \n",
    "    def gradient(s:FMat, a:IMat, y:FMat):Float = {\n",
    "//        \"\"\" Given a state s, action a and target y, compute the model gradients\"\"\"\n",
    "        val ss = formatInput(s); \n",
    "        val (grew, h, h2, h3) = _forward(ss);                             // Run forward\n",
    "        \n",
    "        val rew = FMat(grew);                                            // Compute output gradient on CPU\n",
    "        val dout = y - rew(a + selector);                                // Get the residuals for the actions a \n",
    "        arew.clear;\n",
    "        arew(a + selector) = dout;                                      // The gradient for the actions a\n",
    "        val garew = GMat(arew);                                          // Back to GPU for the rest\n",
    "        \n",
    "        grad(\"W4\").asMat ~ grad(\"W4\").asMat + (h3 *^ garew);              // Backward model gradient (FC)\n",
    "        val dh3 = (model(\"W4\").asMat * garew);                            // Backward data gradient (FC)\n",
    "        dh3 ~ dh3 *@ (h3 > 0)                                            // Map gradients through the RELU\n",
    "        \n",
    "        grad(\"W3\").asMat ~ grad(\"W3\").asMat + (h2.asMat *^ dh3);         // Backward model gradient (FC)\n",
    "        val dh2 = (model(\"W3\").asMat * dh3).asND(nhidden2,10,10,npar);  // Backward data gradient (FC)\n",
    "        dh2 ~ dh2 *@ (h2 > 0)        \n",
    "        \n",
    "        grad(\"W2\").convolveM(h, dh2, false);                             // Backard model gradient (conv)\n",
    "        val dh = model(\"W2\") ^* dh2                                      // Backward data gradient (conv)\n",
    "        dh ~ dh *@ (h > 0)\n",
    "        \n",
    "        grad(\"W1\").convolveM(ss, dh, false);                             // Backward model gradient (conv)\n",
    "\n",
    "        sqrt((dout dotr dout)/dout.length).fv                             // return the RMS residual error. \n",
    "    }\n",
    "    \n",
    "    val lr = GND(1,1,1,1)                                                 // Since this value is changing, it needs to be in a matrix\n",
    "    \n",
    "    def msprop(learning_rate:Float, decay_rate:Float) = {                // MSprop, i.e. RMSprop without the square root.\n",
    "//        \"\"\" Perform model updates from the gradients using MSprop\"\"\"\n",
    "        for ((k,v) <- model) {\n",
    "            val g = grad(k);\n",
    "            val gsq = gradsq(k);\n",
    "            val m = model(k);\n",
    "            val gc = min(max(g, -gclip), gclip);\n",
    "            gsq ~ (gsq * decay_rate) + (g *@ g * (1-decay_rate)); \n",
    "            lr.set(learning_rate);\n",
    "            m ~ m + ((lr *@ g) / (gsq + rmseps));\n",
    "            g.clear;\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement $\\epsilon$-Greedy Policy\n",
    "\n",
    "An $\\epsilon$-Greedy policy should:\n",
    "* with probability $\\epsilon$ take a uniformly-random action.\n",
    "* otherwise choose the best action according to the estimator from the given state.\n",
    "\n",
    "The function below should implement this policy. For each input state, it should return a (column) vector of size nactions which are the probabilities of taking each action. Thus, the probabilities of non-optimal actions should be $\\epsilon/{\\rm nactions}$ and the probability of the best action should be $1-\\epsilon+\\epsilon/{\\rm nactions}$.\n",
    "\n",
    "Since the function processes batches of states, the input <code>state</code> is a <code>nfeatures x nbatch</code> matrix, and the returned value should be a <code>nactions x nbatch</code> matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "val aselector = irow(0->npar)*nactions;\n",
    "val A = zeros(nactions, npar)\n",
    "\n",
    "def policy(estimator:Estimator, state:FMat, epsilon:Float):FMat = {\n",
    "//    \"\"\" Take an estimator and state and predict the best action.\n",
    "//    For each input state, return a vector of action probabilities according to an epsilon-greedy policy\"\"\"\n",
    "    A.set(epsilon / nactions);\n",
    "    val q_values = FMat(estimator.predict(state))\n",
    "    val (_,best_action) = maxi2(q_values)\n",
    "    A(best_action + aselector) = A(best_action + aselector) + (1f - epsilon)\n",
    "    A\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This routine copies the state of one estimator into another. Its used to update the target estimator from the Q-estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "def update_estimator(to_estimator:Estimator, from_estimator:Estimator, window:Int, istep:Int) = {\n",
    "//    \"\"\" every <window> steps, Copy model state from from_estimator into to_estimator\"\"\"\n",
    "    if (istep % window == 0) {\n",
    "        for ((k,v) <- from_estimator.model) {\n",
    "            to_estimator.model(k) <-- from_estimator.model(k);\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement \"Asynchronous Threads\"\n",
    "\n",
    "Don't try that in Python!! Actually all we do here is create an array of environments and advance each one a random number of steps, using random actions at each step. Later on we will make *synchronous* updates to all the environments, but the environments (and their gradient updates) should remain uncorrelated. This serves the same goal as asynchronous updates in paper [2], or experience replay in paper [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing games...92180 steps, 77 epochs in 81.3540 seconds at 0.8826 msecs/step\n"
     ]
    }
   ],
   "source": [
    "// Create estimators\n",
    "val q_estimator = new Estimator(nhidden, nhidden2, nhidden3, nactions)\n",
    "val target_estimator = new Estimator(nhidden, nhidden2, nhidden3, nactions)\n",
    "\n",
    "// The epsilon and learning rate decay schedules\n",
    "// val epsilons = np.linspace(epsilon_start, epsilon_end, neps)\n",
    "val epsilons = epsilon_start / (1f + row(0->neps)/(neps*epsilon_end/epsilon_start))\n",
    "val learning_rates = learning_rate - row(0 -> nlr) * ((lr_end - learning_rate) / nlr)\n",
    "\n",
    "// Initialize the games\n",
    "print(\"Initializing games...\")\n",
    "val envs = new Array[ALE](npar)\n",
    "val state = zeros(nfeats * nwindow, npar)\n",
    "val img0 = zeros(nfeats, 1)\n",
    "var total_time=0f\n",
    "var total_steps=0\n",
    "var total_epochs = 0\n",
    "\n",
    "import java.util.Random\n",
    "val rn = new Random\n",
    "\n",
    "tic\n",
    "for (i <- 0 until npar) {\n",
    "    envs(i) = new ALE\n",
    "    envs(i).setInt(\"random_seed\", i)\n",
    "    envs(i).loadROM(game_bin)\n",
    "\n",
    "    val nmoves = rn.nextInt(init_moves - nwindow) + nwindow\n",
    "    for (j <- 0 until nmoves) {   \n",
    "        val action = VALID_ACTIONS(rn.nextInt(nactions))\n",
    "        val (obs, reward, done) = envs(i).step(action)\n",
    "        total_steps += 1;\n",
    "        if (nmoves - j <= nwindow) {\n",
    "            val k = nwindow - nmoves + j;\n",
    "            state((k*nfeats)->((k+1)*nfeats), i) = preprocess(obs, img0)\n",
    "        }\n",
    "        if (done) {\n",
    "            envs(i).reset() \n",
    "            total_epochs += 1\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "total_time = toc     \n",
    "println(\"%d steps, %d epochs in %5.4f seconds at %5.4f msecs/step\" format(\n",
    "    total_steps, total_epochs, total_time, 1000f*total_time/total_steps))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select integer actions using the probability distribution in each column of <code>probs</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "def random_choices(probs:FMat):IMat = {\n",
    "    val result = izeros(1, probs.ncols);\n",
    "    var i = 0;\n",
    "    while (i < probs.ncols) {\n",
    "        val r = rn.nextFloat();\n",
    "        var j = 0;\n",
    "        var cumprob = probs(0, i);\n",
    "        while (r > cumprob && j+1 < probs.length) {\n",
    "            j += 1;\n",
    "            cumprob += probs(j, i);\n",
    "        }\n",
    "        result(i) = j;\n",
    "        i += 1\n",
    "    }\n",
    "    result\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implement Deep Q-Learning\n",
    "In this cell you actually implement the algorithm. We've given you comments to define all the steps. You should also add book-keeping steps to keep track of the loss, reward and number of epochs (where env.step() returns done = true). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, time 1.9, loss 0.00000244, epochs 0, reward/epoch 0.00000, cum reward/epoch 0.00000\n",
      "step 10000, time 88.6, loss 0.07922183, epochs 136, reward/epoch -20.55882, cum reward/epoch -20.55882\n",
      "step 20000, time 171.7, loss 0.06671724, epochs 254, reward/epoch -20.11864, cum reward/epoch -20.35433\n"
     ]
    }
   ],
   "source": [
    "tic\n",
    "var block_loss = 0f\n",
    "var block_reward = 0f\n",
    "var total_reward = 0f\n",
    "total_epochs = 0\n",
    "var last_epochs = 0\n",
    "val new_state = state.copy\n",
    "Mat.useCache = true;\n",
    "\n",
    "val times = zeros(1,8)\n",
    "val dtimes = zeros(1,7)\n",
    "val rand_actions = int(rand(1, npar) * nactions)\n",
    "val (obs0, rewards0, dones0) = ALE.stepAll2(envs, VALID_ACTIONS(rand_actions))           // step through parallel envs\n",
    "val state_select = irow(nfeats->state.nrows);\n",
    "for (istep <- 0 until nsteps) {\n",
    "//    if (render): envs[0].render()\n",
    "    times(0) = toc\n",
    "    val epsilon = epsilons(math.min(istep, neps-1));                                // get an epsilon for the eps-greedy policy\n",
    "    val lr = learning_rates(math.min(istep, nlr-1));                                // update the decayed learning rate\n",
    "    \n",
    "    update_estimator(target_estimator, q_estimator, target_window, istep);          // update the target estimator if needed    \n",
    "    times(1) = toc\n",
    "    \n",
    "    val action_probs = policy(q_estimator, state, epsilon);                         // get the next action probabilities from the policy\n",
    "    times(2) = toc\n",
    "                                                 \n",
    "    val actions = random_choices(action_probs);                                     // Choose actions using the policy\n",
    "    val (obs, rewards, dones) = ALE.stepAll2(envs, VALID_ACTIONS(actions), obs0, rewards0, dones0)           // step through parallel envs\n",
    "    times(3) = toc\n",
    "    \n",
    "    for (i <- 0 until npar) {                                                     \n",
    "        val img = preprocess(obs(i), img0);                                               // process the observation\n",
    "        new_state(?,i) = state(state_select,i) on img;                       // add it to buffer of last nwindow imgs        \n",
    "    }    \n",
    "    total_epochs += sum(dones).v.toInt\n",
    "    block_reward += sum(rewards).v  \n",
    "    times(4) = toc\n",
    "    \n",
    "    val q_values_next = target_estimator.predict(new_state);                        // predict the Q values\n",
    "    times(5) = toc\n",
    "    \n",
    "    dones <-- (dones + (rewards != 0f) > 0f);\n",
    "    val targets = rewards+discount_factor*(1f-dones) *@ maxi(q_values_next);        // compute target values\n",
    "    block_loss += q_estimator.gradient(state, actions, targets);                    // compute q-estimator gradient and return the loss\n",
    "    times(6) = toc\n",
    "     \n",
    "    if (istep % rmsevery == 0) {\n",
    "       q_estimator.msprop(lr, gsq_decay);                       // apply the gradient update\n",
    "//        print(\"ds1=%f, ss1=%f, ds2=%f, ss2=%f\\n\" format(res(0,0), res(1,0), res(0,1), res(1,1)));\n",
    "    }\n",
    "    times(7) = toc\n",
    "    \n",
    "    dtimes ~ dtimes + (times(0,1->8) - times(0,0->7))\n",
    "    val t = toc;\n",
    "    if (istep % printsteps == 0) {\n",
    "        total_reward += block_reward;\n",
    "        println(\"step %d, time %2.1f, loss %9.8f, epochs %d, reward/epoch %6.5f, cum reward/epoch %6.5f\" format(\n",
    "                istep, t, block_loss/printsteps, total_epochs, block_reward/math.max(1,total_epochs-last_epochs), total_reward/math.max(1,total_epochs)))\n",
    "        last_epochs = total_epochs;\n",
    "        block_reward = 0f;\n",
    "        block_loss = 0f;\n",
    "    }\n",
    "    state <-- new_state;\n",
    "}\n",
    "dtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val s=col(0->25600)*ones(1,16)\n",
    "val ss = GMat(s).asND(nwindow,height,width,npar);\n",
    "val h = q_estimator.model(\"W1\") * ss\n",
    "val h2 = q_estimator.model(\"W2\") * h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Mat.useCache=false\n",
    "val s=col(0->25600)*ones(1,2)\n",
    "val a=2*iones(1,npar)\n",
    "val y=ones(1,npar)\n",
    "val selector = irow(0->npar)*nactions;\n",
    "val farew = zeros(nactions, npar);\n",
    "q_estimator.model(\"W1\")\n",
    "val (grew,h,h2,h3) = q_estimator.forward(s)\n",
    "/*val rew = FMat(grew);                                            // Compute output gradient on CPU\n",
    "val dout = y - rew(a + selector); \n",
    "\n",
    "farew.clear;\n",
    "farew(a + selector) = dout;                                      // The gradient for the actions a\n",
    "val arew = GMat(farew); \n",
    "q_estimator.grad.map({case (k,v)=>v.clear})\n",
    "val v = q_estimator.gradient(s,a,y)\n",
    "h3 *^ arew */\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_estimator.predict(ones(25600,2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And animate the model's performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_estimator.grad(\"W4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val (a,b,c,d) = q_estimator.forward(col(0->25600)*ones(1,2));\n",
    "d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "/*state0 = state[:,0]\n",
    "for i in np.arange(200):\n",
    "    envs[0].render()\n",
    "    preds = test_estimator.predict(state0)\n",
    "    iaction = np.argmax(preds)\n",
    "    obs, _, done0, _ = envs[0].step(VALID_ACTIONS[iaction])\n",
    "    state0 = np.concatenate((state0[nfeats:], preprocess(obs)))\n",
    "    if (done0): envs[0].reset() */\n",
    "//hist(ln(abs(q_estimator.gradsq(\"W1\")(?))),100)\n",
    "//    hist(ln(abs(q_estimator.gradsq(\"W1\")(?))),100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there we have it. Simple 1-step Q-Learning can solve easy problems very fast. Note that environments that produce images will be much slower to train on than environments (like CartPole) which return an observation of the state of the system. But this model can still train on those image-based games - like Atari games. It will take hours-days however. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//q_estimator.gradient(state,actions,targets)\n",
    "q_estimator.model(\"W2\").dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val f=FFilter.FFilter2Ddn(7,7,4,16,4,3);\n",
    "f.dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val test_steps = 100001;\n",
    "val testprintsteps = 10000;\n",
    "\n",
    "val test_estimator = new Estimator(nfeats*nwindow, nhidden, nactions);\n",
    "test_estimator.model(\"W1\") = loadFMat(\"BestW1.fmat.lz4\");\n",
    "test_estimator.model(\"W2\") = loadFMat(\"BestW2.fmat.lz4\");\n",
    "\n",
    "block_reward = 0f;\n",
    "total_reward = 0f;\n",
    "total_epochs = 0;\n",
    "last_epochs = 0;\n",
    "\n",
    "tic;\n",
    "for (istep <- 0 until test_steps) {\n",
    "    \n",
    "    val action_probs = policy(test_estimator, state, 0);                    // get the next action probabilities from the policy                                                         \n",
    "    val actions = random_choices(action_probs);                             // Choose actions using the policy\n",
    "    val (obs, rewards, dones) = ALE.stepAll2(envs, VALID_ACTIONS(actions))  // step through parallel envs   \n",
    "    for (i <- 0 until npar) {                                                     \n",
    "        val img = preprocess(obs(i));                                       // process the observation\n",
    "        state(?,i) = state(nfeats->state.nrows,i) on img;                   // add it to buffer of last nwindow imgs        \n",
    "    }    \n",
    "    total_epochs += sum(dones).v.toInt\n",
    "    block_reward += sum(rewards).v  \n",
    "    \n",
    "    val t = toc;\n",
    "    if (istep % testprintsteps == 0) {\n",
    "        total_reward += block_reward;\n",
    "        println(\"step %d, time %2.1f, epochs %d, reward/epoch %6.5f, cum reward/epoch %6.5f\" format(\n",
    "                istep, t, total_epochs, block_reward/math.max(1,total_epochs-last_epochs), total_reward/math.max(1,total_epochs)))\n",
    "        last_epochs = total_epochs;\n",
    "        block_reward = 0f;\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "name": "scala",
   "version": "2.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
