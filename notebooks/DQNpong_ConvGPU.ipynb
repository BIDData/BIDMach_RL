{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning\n",
    "\n",
    "In this notebook, you will implement a deep Q-Learning reinforcement algorithm. The implementation borrows ideas from both the original DeepMind Nature paper and the more recent asynchronous version:<br/>\n",
    "[1] \"Human-Level Control through Deep Reinforcement Learning\" by Mnih et al. 2015<br/>\n",
    "[2] \"Asynchronous Methods for Deep Reinforcement Learning\" by Mnih et al. 2016.<br/>\n",
    "\n",
    "In particular:\n",
    "* We use separate target and Q-functions estimators with periodic updates to the target estimator. \n",
    "* We use several concurrent \"threads\" rather than experience replay to generate less biased gradient updates. \n",
    "* Threads are actually synchronized so we start each one at a random number of moves.\n",
    "* We use an epsilon-greedy policy that blends random moves with policy moves.\n",
    "* We taper the random search parameter (epsilon) and the learning rate to zero during training.\n",
    "\n",
    "This gives a simple and reasonably fast general-purpose RL algorithm. We use it here for the Cartpole environment from OpenAI Gym, but it can easily be adapted to others. For this notebook, you will implement 4 steps:\n",
    "\n",
    "1. The backward step for the Q-estimator\n",
    "2. The $\\epsilon$-greedy policy\n",
    "3. \"asynchronous\" initialization \n",
    "4. The Q-learning algorithm\n",
    "\n",
    "To get started, we import some prerequisites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 CUDA device found, CUDA version 8.0\n"
     ]
    }
   ],
   "source": [
    "import BIDMat.{CMat,CSMat,DMat,Dict,FMat,FFilter,Filter,GFilter,GMat,GDMat,GIMat,GLMat,GSMat,GSDMat,HMat,IDict,Image,IMat,LMat,Mat,SMat,SBMat,SDMat,TMat}\n",
    "import BIDMat.MatFunctions._\n",
    "import BIDMat.SciFunctions._\n",
    "import BIDMat.Solvers._\n",
    "import BIDMat.JPlotting._\n",
    "import BIDMach.Learner\n",
    "\n",
    "import BIDMach.models.{Click,FM,GLM,KMeans,KMeansw,LDA,LDAgibbs,Model,NMF,SFA,RandomForest,SVD}\n",
    "import BIDMach.networks.{Net}\n",
    "import BIDMach.datasources.{DataSource,MatSource,FileSource,SFileSource}\n",
    "import BIDMach.datasinks.{DataSink,MatSink}\n",
    "import BIDMach.mixins.{CosineSim,Perplexity,Top,L1Regularizer,L2Regularizer}\n",
    "import BIDMach.updaters.{ADAGrad,Batch,BatchNorm,Grad,IncMult,IncNorm,Telescoping}\n",
    "import BIDMach.causal.{IPTW}\n",
    "import BIDMach.rl.ALE\n",
    "\n",
    "Mat.checkMKL(false)\n",
    "Mat.checkCUDA\n",
    "Mat.setInline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below lists some parameters you can tune. Most should be self-explanatory. They are currently set to train Pong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nsteps = 2000001                     // Number of steps to run (game actions per environment)\n",
    "val npar = 16                            // Number of parallel environments\n",
    "val target_window = 50                   // Interval to update target estimator from q-estimator\n",
    "val discount_factor = 0.99f              // Reward discount factor\n",
    "val printsteps = 10000                   // Number of steps between printouts\n",
    "val render = false                       // Whether to render an environment while training\n",
    "\n",
    "val epsilon_start = 0.3f                 // Parameters for epsilon-greedy policy: initial epsilon\n",
    "val epsilon_end = 0.1f                   // Final epsilon\n",
    "val neps = (0.9*nsteps).toInt            // Number of steps to decay epsilon\n",
    "\n",
    "//val learning_rate = 1e-5f\n",
    "val learning_rate = 3e-6f                // Initial learning rate\n",
    "val lr_end = learning_rate               // Final learning rate\n",
    "val nlr = neps                           // Steps to decay learning rate\n",
    "val gsq_decay = 0.99f                    // Decay factor for MSProp\n",
    "val vel_decay = 0.9f                     // Momentum decay\n",
    "val gclip = 1f                           // gradient clipping\n",
    "val rmsevery = 5                         // apply gradient after skipping this many steps\n",
    "\n",
    "val nhidden = 16                         // Number of hidden layers for estimators\n",
    "val nhidden2 = 32\n",
    "val nhidden3 = 256\n",
    "\n",
    "val init_moves = 10000                   // Upper bound on random number of moves to take initially\n",
    "val nwindow = 4                          // Sensing window = last n images in a state\n",
    "\n",
    "// The epsilon and learning rate decay schedules\n",
    "// val epsilons = np.linspace(epsilon_start, epsilon_end, neps)\n",
    "\n",
    "val epsilons = epsilon_start / (1f + row(0->neps)/(neps*epsilon_end/epsilon_start))\n",
    "val learning_rates = learning_rate - row(0 -> nlr) * ((lr_end - learning_rate) / nlr)\n",
    "\n",
    "\n",
    "// Model type and action definitions\n",
    "\n",
    "val game_bin=\"/code/ALE/roms/Pong.bin\"                 \n",
    "val VALID_ACTIONS = irow(0, 3, 4)\n",
    "val nactions= VALID_ACTIONS.length\n",
    "val nfeats = 80*80  \n",
    "val height = 80\n",
    "val width = 80\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are environment-specific parameters. The function \"preprocess\" should process an observation returned by the environment into a vector for training. For CartPole we simply append a 1 to implement bias in the first layer. \n",
    "\n",
    "For visual environments you would typically crop, downsample to 80x80, set color to a single bit (foreground/background), and flatten to a vector. \n",
    "\n",
    "*nfeats* is the dimension of the vector output by *preprocess*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "\n",
    "def preprocess(I:Array[Byte], out:FMat):FMat = {\n",
    "//  Preprocess Pong game frames into vectors.\n",
    "//  Input:\n",
    "//    - (3,160,210) uint8 frame representing Pong game screen.\n",
    "//  Returns:\n",
    "//    - Downsampled (DxD) matrix of 0s and 1s.\n",
    "    var i = 0;\n",
    "    val res = if (out.asInstanceOf[AnyRef] == null) zeros(width\\height\\1\\1) else out;\n",
    "    while (i < 80) {\n",
    "        var j = 0;\n",
    "        while (j < 80) {\n",
    "            val x = I(j*2 + 160 * (i*2 + 35));\n",
    "            res.data(j + 80*i) = {if (x == 34) 0f else {if (x != 0) 1f else 0f}};\n",
    "            j += 1;\n",
    "        }\n",
    "        i += 1;\n",
    "    }\n",
    "    res\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the Q-estimator class. We use two instances of this class, one for the target estimator, and one for the Q-estimator. The Q function is normally represented as a scalar $Q(x,a)$ where $x$ is the state and $a$ is an action. For ease of implementation, we actually estimate a vector-valued function $Q(x,.)$ which returns the estimated reward for every action. The model here has just a single hidden layer:\n",
    "\n",
    "<pre>\n",
    "Input Layer (nfeats) => FC Layer => RELU => FC Layer => Output (naction values)\n",
    "</pre>\n",
    "\n",
    "## 1. Implement Q-estimator gradient\n",
    "Your first task is to implement the\n",
    "<pre>Estimator.gradient(s, a, y)</pre>\n",
    "method for this class. **gradient** should compute the gradients wrt weight arrays W1 and W2 into\n",
    "<pre>self.grad['W1']\n",
    "self.grad['W2']</pre>\n",
    "respectively. Both <code>a</code> and <code>y</code> are vectors. Be sure to update only the output layer weights corresponding to the given action vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "\n",
    "import jcuda.jcudnn.cudnnTensorFormat._\n",
    "import jcuda.jcudnn.cudnnConvolutionMode._\n",
    "\n",
    "val convt = CUDNN_CROSS_CORRELATION;\n",
    "val tformat = Net.TensorNCHW;\n",
    "\n",
    "class Myopts extends Net.Options with ADAGrad.Opts;\n",
    "\n",
    "class Estimator(nhidden:Int, nhidden2:Int, nhidden3:Int, nactions:Int) {\n",
    "    \n",
    "    val opts = new Myopts;  \n",
    "    opts.vexp = 1f;\n",
    "    opts.texp = 0f;\n",
    "    opts.waitsteps = -1;\n",
    "    opts.vel_decay = vel_decay;\n",
    "    opts.gsq_decay = gsq_decay;\n",
    "    opts.tensorFormat = tformat;\n",
    "    \n",
    "    import BIDMach.networks.layers.Node._;\n",
    "\n",
    "    val in =    input;\n",
    "    val acts =  input;\n",
    "    \n",
    "    val conv1 = conv(in)(w=7,h=7,nch=nhidden,stride=4,pad=3,initv=1f,convType=convt);\n",
    "    val relu1 = relu(conv1);\n",
    "\n",
    "    val conv2 = conv(relu1)(w=3,h=3,nch=nhidden2,stride=2,pad=1,convType=convt);\n",
    "    val relu2 = relu(conv2);\n",
    "\n",
    "    val fc3 =   linear(relu2)(outdim=nhidden3,initv=2e-2f);\n",
    "    val relu3 = relu(fc3);\n",
    "    \n",
    "    val fc4 =   linear(relu3)(outdim=nactions,initv=5e-2f); \n",
    "    val npredict = 9;                                       // Number of layers for action prediction. \n",
    "    \n",
    "    val sel =   fc4(acts);\n",
    "    \n",
    "    val out =   glm(sel)(izeros(1,1))\n",
    "\n",
    "    val nodes = (in     \\ acts   on\n",
    "                 conv1  \\ relu1  on\n",
    "                 conv2  \\ relu2  on\n",
    "                 fc3    \\ relu3  on\n",
    "                 fc4    \\ null   on\n",
    "                 sel    \\ out    ).t\n",
    "\n",
    "    opts.nodemat = nodes;\n",
    "    \n",
    "    val net = new Net(opts);\n",
    "    val adagrad = new ADAGrad(opts);\n",
    "    \n",
    "    def formatStates(s:FMat):FMat = {\n",
    "        if (tformat == Net.TensorNCHW) {\n",
    "            s.reshapeView(nwindow\\height\\width\\npar);\n",
    "        } else {\n",
    "            s.reshapeView(height\\width\\nwindow\\npar).transpose(2\\0\\1\\3);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "/** Perform the initialization that is normally done by the Learner */\n",
    "\n",
    "    var initialized = false;\n",
    "    \n",
    "    def checkinit(states:FMat, actions:IMat, rewards:FMat) = {\n",
    "    if (net.mats.asInstanceOf[AnyRef] == null) {\n",
    "        net.mats = new Array[Mat](3);\n",
    "        net.gmats = new Array[Mat](3);\n",
    "    }\n",
    "    net.mats(0) = states;\n",
    "    if (net.mats(1).asInstanceOf[AnyRef] == null) {\n",
    "        net.mats(1) = izeros(1, states.ncols);              // Dummy action vector\n",
    "        net.mats(2) = zeros(1, states.ncols);               // Dummy reward vector\n",
    "    }\n",
    "    if (actions.asInstanceOf[AnyRef] != null) {\n",
    "        net.mats(1) <-- actions;\n",
    "    }\n",
    "    if (rewards.asInstanceOf[AnyRef] != null) {\n",
    "        net.mats(2) <-- rewards;\n",
    "    }\n",
    "    if (!initialized) {\n",
    "        net.useGPU = (opts.useGPU && Mat.hasCUDA > 0);\n",
    "        net.init();\n",
    "        adagrad.init(net);\n",
    "        initialized = true;\n",
    "    }\n",
    "    net.copyMats(net.mats, net.gmats);\n",
    "    net.assignInputs(net.gmats, 0, 0);\n",
    "    net.assignTargets(net.gmats, 0, 0);\n",
    "    }\n",
    "    \n",
    "/**  Run the model forward given a state as input up to the action prediction layer. \n",
    "     Action selection/scoring layers are not updated.\n",
    "     returns action predictions */\n",
    "    \n",
    "    def predict(states:FMat):FMat = {\n",
    "        val fstates = formatStates(states);\n",
    "        checkinit(fstates, null, null);\n",
    "        for (i <- 0 until npredict) net.layers(i).forward;\n",
    "        FMat(net.layers(npredict-1).output);\n",
    "    }\n",
    "\n",
    "/** Run the model all the way forward to the squared loss output layer, \n",
    "    and then backward to compute gradients.\n",
    "    An action vector and reward vector must be given. */  \n",
    "    \n",
    "    def gradient(states:FMat, actions:IMat, rewards:FMat):Float = {\n",
    "        val fstates = formatStates(states);\n",
    "        checkinit(fstates, actions, rewards);\n",
    "        net.forward;\n",
    "        net.backward(0, 0);\n",
    "        val dout = net.layers(net.layers.length-1).score;\n",
    "        sqrt(-dout/dout.length).fv                             // return the RMS residual error. \n",
    "    }\n",
    "        \n",
    "/** MSprop, i.e. RMSprop without the square root, or natural gradient */\n",
    "        \n",
    "    def msprop(learning_rate:Float) = {                \n",
    "        opts.lrate = learning_rate;\n",
    "        adagrad.update(0,0,0);\n",
    "        net.cleargrad;\n",
    "    }\n",
    "\n",
    "    val aselector = irow(0->npar)*nactions;\n",
    "    val A = zeros(nactions, npar);\n",
    "\n",
    "    def policy(state:FMat, epsilon:Float):FMat = {\n",
    "//    \"\"\" Take an estimator and state and predict the best action.\n",
    "//    For each input state, return a vector of action probabilities according to an epsilon-greedy policy\"\"\"\n",
    "        A.set(epsilon / nactions);\n",
    "        val q_values = predict(state);\n",
    "        val (_,best_action) = maxi2(q_values)\n",
    "        A(best_action + aselector) = A(best_action + aselector) + (1f - epsilon)\n",
    "        A\n",
    "    }\n",
    "\n",
    "    def update_estimator(to_estimator:Estimator, window:Int, istep:Int) = {\n",
    "//    \"\"\" every <window> steps, Copy model state from from_estimator into to_estimator\"\"\"\n",
    "        if (istep % window == 0) {\n",
    "            for (k  <- 0 until net.modelmats.length) {\n",
    "                to_estimator.net.modelmats(k) <-- net.modelmats(k);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "};\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement \"Asynchronous Threads\"\n",
    "\n",
    "Don't try that in Python!! Actually all we do here is create an array of environments and advance each one a random number of steps, using random actions at each step. Later on we will make *synchronous* updates to all the environments, but the environments (and their gradient updates) should remain uncorrelated. This serves the same goal as asynchronous updates in paper [2], or experience replay in paper [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing games...75791 steps, 58 epochs in 81.1140 seconds at 1.0702 msecs/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "// Initialize the games\n",
    "print(\"Initializing games...\")\n",
    "val envs = new Array[ALE](npar)\n",
    "val state = zeros(width\\height\\nwindow\\npar)\n",
    "val img0 = zeros(width\\height\\1\\1)\n",
    "var total_time=0f\n",
    "var total_steps=0\n",
    "var total_epochs = 0\n",
    "\n",
    "import java.util.Random\n",
    "val rn = new Random\n",
    "\n",
    "tic\n",
    "for (i <- 0 until npar) {\n",
    "    envs(i) = new ALE\n",
    "    envs(i).setInt(\"random_seed\", i)\n",
    "    envs(i).loadROM(game_bin)\n",
    "\n",
    "    val nmoves = rn.nextInt(init_moves - nwindow) + nwindow\n",
    "    for (j <- 0 until nmoves) {   \n",
    "        val action = VALID_ACTIONS(rn.nextInt(nactions))\n",
    "        val (obs, reward, done) = envs(i).step2(action)\n",
    "        total_steps += 1;\n",
    "        if (nmoves - j <= nwindow) {\n",
    "            val k = nwindow - nmoves + j;\n",
    "            state(?, ?, k->(k+1), i) = preprocess(obs, img0)\n",
    "        }\n",
    "        if (done) {\n",
    "            envs(i).reset() \n",
    "            total_epochs += 1\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "total_time = toc     \n",
    "println(\"%d steps, %d epochs in %5.4f seconds at %5.4f msecs/step\" format(\n",
    "    total_steps, total_epochs, total_time, 1000f*total_time/total_steps))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select integer actions using the probability distribution in each column of <code>probs</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "def random_choices(probs:FMat):IMat = {\n",
    "    val result = izeros(1, probs.ncols);\n",
    "    var i = 0;\n",
    "    while (i < probs.ncols) {\n",
    "        val r = rn.nextFloat();\n",
    "        var j = 0;\n",
    "        var cumprob = probs(0, i);\n",
    "        while (r > cumprob && j+1 < probs.length) {\n",
    "            j += 1;\n",
    "            cumprob += probs(j, i);\n",
    "        }\n",
    "        result(i) = j;\n",
    "        i += 1\n",
    "    }\n",
    "    result\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implement Deep Q-Learning\n",
    "In this cell you actually implement the algorithm. We've given you comments to define all the steps. You should also add book-keeping steps to keep track of the loss, reward and number of epochs (where env.step() returns done = true). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, time 0.0, loss 0.00000162, epochs 0, reward/epoch 0.00000, cum reward/epoch 0.00000\n",
      "step 10000, time 110.2, loss 0.06295285, epochs 145, reward/epoch -20.38621, cum reward/epoch -20.38621\n",
      "step 20000, time 219.2, loss 0.04919356, epochs 281, reward/epoch -20.51471, cum reward/epoch -20.44840\n",
      "step 30000, time 327.9, loss 0.05800981, epochs 387, reward/epoch -19.32076, cum reward/epoch -20.13953\n",
      "step 40000, time 436.2, loss 0.05846595, epochs 478, reward/epoch -19.59341, cum reward/epoch -20.03556\n",
      "step 50000, time 544.4, loss 0.06574187, epochs 561, reward/epoch -18.80723, cum reward/epoch -19.85383\n"
     ]
    }
   ],
   "source": [
    "// Create estimators\n",
    "val q_estimator = new Estimator(nhidden, nhidden2, nhidden3, nactions)\n",
    "val target_estimator = new Estimator(nhidden, nhidden2, nhidden3, nactions)\n",
    "\n",
    "// Initialize them by making predictions\n",
    "q_estimator.predict(state);\n",
    "target_estimator.predict(state);\n",
    "\n",
    "tic\n",
    "var block_loss = 0f\n",
    "var block_reward = 0f\n",
    "var total_reward = 0f\n",
    "total_epochs = 0\n",
    "var last_epochs = 0\n",
    "val new_state = state.copy\n",
    "Mat.useGPUcache = true;\n",
    "\n",
    "val times = zeros(1,8);\n",
    "val dtimes = zeros(1,7);\n",
    "val rand_actions = int(rand(1, npar) * nactions);\n",
    "val (obs0, rewards0, dones0) = ALE.stepAll2(envs, VALID_ACTIONS(rand_actions));           // step through parallel envs\n",
    "var actions:IMat = null;\n",
    "var action_probs:FMat = null;\n",
    "\n",
    "for (istep <- 0 until nsteps) {\n",
    "//    if (render): envs[0].render()\n",
    "    times(0) = toc\n",
    "    val epsilon = epsilons(math.min(istep, neps-1));                                // get an epsilon for the eps-greedy policy\n",
    "    val lr = learning_rates(math.min(istep, nlr-1));                                // update the decayed learning rate\n",
    "    \n",
    "    q_estimator.update_estimator(target_estimator, target_window, istep);          // update the target estimator if needed    \n",
    "    times(1) = toc\n",
    "    \n",
    "    action_probs = q_estimator.policy(state, epsilon);                         // get the next action probabilities from the policy\n",
    "    times(2) = toc\n",
    "                                                 \n",
    "    actions = random_choices(action_probs);                                     // Choose actions using the policy\n",
    "    val (obs, rewards, dones) = ALE.stepAll2(envs, VALID_ACTIONS(actions), obs0, rewards0, dones0)           // step through parallel envs\n",
    "    times(3) = toc\n",
    "    \n",
    "    for (i <- 0 until npar) {                                                     \n",
    "        val img = preprocess(obs(i), img0);                                               // process the observation\n",
    "        new_state(?,?,0->(nwindow-1),i) = state(?,?,1->nwindow,i);              // shift the image stack and add a new image\n",
    "        new_state(?,?,nwindow-1,i) = img;         \n",
    "    }    \n",
    "    total_epochs += sum(dones).v.toInt\n",
    "    block_reward += sum(rewards).v  \n",
    "    times(4) = toc\n",
    "    \n",
    "    val q_values_next = target_estimator.predict(new_state);                        // predict the Q values\n",
    "    times(5) = toc\n",
    "    \n",
    "    dones <-- (dones + (rewards != 0f) > 0f);\n",
    "    val targets = rewards+discount_factor*(1f-dones) *@ maxi(q_values_next);        // compute target values\n",
    "    block_loss += q_estimator.gradient(state, actions, targets);                    // compute q-estimator gradient and return the loss\n",
    "    times(6) = toc\n",
    "     \n",
    "    if (istep % rmsevery == 0) {\n",
    "       q_estimator.msprop(lr);                       // apply the gradient update\n",
    "//        print(\"ds1=%f, ss1=%f, ds2=%f, ss2=%f\\n\" format(res(0,0), res(1,0), res(0,1), res(1,1)));\n",
    "    }\n",
    "    times(7) = toc\n",
    "    \n",
    "    dtimes ~ dtimes + (times(0,1->8) - times(0,0->7))\n",
    "    val t = toc;\n",
    "    if (istep % printsteps == 0) {\n",
    "        total_reward += block_reward;\n",
    "        println(\"step %d, time %2.1f, loss %9.8f, epochs %d, reward/epoch %6.5f, cum reward/epoch %6.5f\" format(\n",
    "                istep, t, block_loss/printsteps, total_epochs, block_reward/math.max(1,total_epochs-last_epochs), total_reward/math.max(1,total_epochs)))\n",
    "        last_epochs = total_epochs;\n",
    "        block_reward = 0f;\n",
    "        block_loss = 0f;\n",
    "    }\n",
    "    state <-- new_state;\n",
    "    //    Mat.debugMem = true;\n",
    "    //    Mat.debugMemThreshold = 0;\n",
    "}\n",
    "dtimes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "name": "scala",
   "version": "2.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
