{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning\n",
    "\n",
    "In this notebook, you will implement a deep Q-Learning reinforcement algorithm. The implementation borrows ideas from both the original DeepMind Nature paper and the more recent asynchronous version:<br/>\n",
    "[1] \"Human-Level Control through Deep Reinforcement Learning\" by Mnih et al. 2015<br/>\n",
    "[2] \"Asynchronous Methods for Deep Reinforcement Learning\" by Mnih et al. 2016.<br/>\n",
    "\n",
    "In particular:\n",
    "* We use separate target and Q-functions estimators with periodic updates to the target estimator. \n",
    "* We use several concurrent \"threads\" rather than experience replay to generate less biased gradient updates. \n",
    "* Threads are actually synchronized so we start each one at a random number of moves.\n",
    "* We use an epsilon-greedy policy that blends random moves with policy moves.\n",
    "* We taper the random search parameter (epsilon) and the learning rate to zero during training.\n",
    "\n",
    "This gives a simple and reasonably fast general-purpose RL algorithm. We use it here for the Cartpole environment from OpenAI Gym, but it can easily be adapted to others. For this notebook, you will implement 4 steps:\n",
    "\n",
    "1. The backward step for the Q-estimator\n",
    "2. The $\\epsilon$-greedy policy\n",
    "3. \"asynchronous\" initialization \n",
    "4. The Q-learning algorithm\n",
    "\n",
    "To get started, we import some prerequisites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 CUDA device found, CUDA version 8.0\n"
     ]
    }
   ],
   "source": [
    "import BIDMat.{CMat,CSMat,DMat,Dict,FMat,FND,FFilter,Filter,GFilter,GMat,GDMat,GIMat,GLMat,GSMat,GSDMat,GND,HMat,IDict,Image,IMat,LMat,Mat,SMat,SBMat,SDMat,TMat}\n",
    "import BIDMat.MatFunctions._\n",
    "import BIDMat.SciFunctions._\n",
    "import BIDMat.Solvers._\n",
    "import BIDMat.JPlotting._\n",
    "import BIDMach.Learner\n",
    "\n",
    "import BIDMach.models.{Click,FM,GLM,KMeans,KMeansw,LDA,LDAgibbs,Model,NMF,SFA,RandomForest,SVD}\n",
    "import BIDMach.networks.{Net}\n",
    "import BIDMach.datasources.{DataSource,MatSource,FileSource,SFileSource}\n",
    "import BIDMach.datasinks.{DataSink,MatSink}\n",
    "import BIDMach.mixins.{CosineSim,Perplexity,Top,L1Regularizer,L2Regularizer}\n",
    "import BIDMach.updaters.{ADAGrad,Batch,BatchNorm,Grad,IncMult,IncNorm,Telescoping}\n",
    "import BIDMach.causal.{IPTW}\n",
    "import BIDMach.rl.ALE\n",
    "\n",
    "Mat.checkMKL(false)\n",
    "Mat.checkCUDA\n",
    "Mat.setInline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ale = new ALE\n",
    "ale.getFloat(\"repeat_action_probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below lists some parameters you can tune. They should be self-explanatory. They are currently set to train CartPole-V0 to a \"solved\" score (> 195) most of the time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nsteps = 20000001                    // Number of steps to run (game actions per environment)\n",
    "val npar = 16                            // Number of parallel environments\n",
    "val target_window = 100                  // Interval to update target estimator from q-estimator\n",
    "val discount_factor = 0.99f              // Reward discount factor\n",
    "val printsteps = 10000                   // Number of steps between printouts\n",
    "val render = false                       // Whether to render an environment while training\n",
    "\n",
    "val epsilon_start = 0.5f                 // Parameters for epsilon-greedy policy: initial epsilon\n",
    "val epsilon_end = 0.1f                  // Final epsilon\n",
    "val neps = (0.9*nsteps).toInt            // Number of steps to decay epsilon\n",
    "\n",
    "//val learning_rate = 5e-4f\n",
    "val learning_rate = 3e-6f                // Initial learning rate\n",
    "val lr_end = learning_rate               // Final learning rate\n",
    "val nlr = neps                           // Steps to decay learning rate\n",
    "val gsq_decay = 0.99f                   // Decay factor for RMSProp\n",
    "val momentum_decay = 0.9f\n",
    "val gclip = 1f\n",
    "val rmseps = 1e-5f\n",
    "val rmsevery = 5\n",
    "\n",
    "val nhidden = 16                         // Number of hidden layers for estimators\n",
    "val nhidden2 = 32\n",
    "val nhidden3 = 256\n",
    "\n",
    "val init_moves = 10000                   // Upper bound on random number of moves to take initially\n",
    "val nwindow = 4                          // Sensing window = last n images in a state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are environment-specific parameters. The function \"preprocess\" should process an observation returned by the environment into a vector for training. For CartPole we simply append a 1 to implement bias in the first layer. \n",
    "\n",
    "For visual environments you would typically crop, downsample to 80x80, set color to a single bit (foreground/background), and flatten to a vector. \n",
    "\n",
    "*nfeats* is the dimension of the vector output by *preprocess*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "val game_bin=\"/code/ALE/roms/Pong.bin\"                 // Model type and action definitions\n",
    "val VALID_ACTIONS = irow(0, 3, 4)\n",
    "val nactions= VALID_ACTIONS.length\n",
    "val nfeats = 80*80  \n",
    "val height = 80\n",
    "val width = 80\n",
    "\n",
    "def preprocess(I:FND):FMat = {\n",
    "//  Preprocess Pong game frames into vectors.\n",
    "//  Input:\n",
    "//    - (3,160,210) uint8 frame representing Pong game screen.\n",
    "//  Returns:\n",
    "//    - Downsampled (DxD) matrix of 0s and 1s, \"raveled\" into a 1-D vector.\n",
    "    var i = 0;\n",
    "    val res = zeros(80*80,1)\n",
    "    while (i < 80) {\n",
    "        var j = 0;\n",
    "        while (j < 80) {\n",
    "            val x = I.data(3*(j*2 + 160 * (i*2 + 35)));\n",
    "            res.data(j + 80*i) = {if (x == 144f || x == 109f) 0f else {if (x != 0f) 1f else 0f}};\n",
    "            j += 1;\n",
    "        }\n",
    "        i += 1;\n",
    "    }\n",
    "    res\n",
    "}\n",
    "\n",
    "def preprocess(I:Array[Byte]):FMat = {\n",
    "//  Preprocess Pong game frames into vectors.\n",
    "//  Input:\n",
    "//    - (3,160,210) uint8 frame representing Pong game screen.\n",
    "//  Returns:\n",
    "//    - Downsampled (DxD) matrix of 0s and 1s, \"raveled\" into a 1-D vector.\n",
    "    var i = 0;\n",
    "    val res = zeros(80*80,1)\n",
    "    while (i < 80) {\n",
    "        var j = 0;\n",
    "        while (j < 80) {\n",
    "            val x = I(j*2 + 160 * (i*2 + 35));\n",
    "            res.data(j + 80*i) = {if (x == 34) 0f else {if (x != 0) 1f else 0f}};\n",
    "            j += 1;\n",
    "        }\n",
    "        i += 1;\n",
    "    }\n",
    "    res\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the Q-estimator class. We use two instances of this class, one for the target estimator, and one for the Q-estimator. The Q function is normally represented as a scalar $Q(x,a)$ where $x$ is the state and $a$ is an action. For ease of implementation, we actually estimate a vector-valued function $Q(x,.)$ which returns the estimated reward for every action. The model here has just a single hidden layer:\n",
    "\n",
    "<pre>\n",
    "Input Layer (nfeats) => FC Layer => RELU => FC Layer => Output (naction values)\n",
    "</pre>\n",
    "\n",
    "## 1. Implement Q-estimator gradient\n",
    "Your first task is to implement the\n",
    "<pre>Estimator.gradient(s, a, y)</pre>\n",
    "method for this class. **gradient** should compute the gradients wrt weight arrays W1 and W2 into\n",
    "<pre>self.grad['W1']\n",
    "self.grad['W2']</pre>\n",
    "respectively. Both <code>a</code> and <code>y</code> are vectors. Be sure to update only the output layer weights corresponding to the given action vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import scala.collection.mutable.HashMap;\n",
    "\n",
    "class Estimator(nhidden:Int, nhidden2:Int, nhidden3:Int, nactions:Int) {\n",
    "\n",
    "//        \"\"\" Create model matrices, and gradient and squared gradient buffers\"\"\"\n",
    "    val model = new HashMap[String, FFilter]();                 // Store model params in Filters, even for FC layers\n",
    "    val grad = new HashMap[String, FFilter]();\n",
    "    val gradsq = new HashMap[String, FFilter]();\n",
    "    model(\"W1\") = FFilter.FFilter2Ddn(7,7,nwindow,nhidden,4,3);                         // First conv layer\n",
    "    model(\"W1\")(?) = normrnd(0,1,49*nwindow*nhidden,1) / math.sqrt(49*nwindow).toFloat  // \"Xavier\" initialization\n",
    "    model(\"W2\") = FFilter.FFilter2Ddn(3,3,nhidden,nhidden2,2,1);                        // Second conv layer\n",
    "    model(\"W2\")(?) = normrnd(0,1,9*nhidden*nhidden2,1) / math.sqrt(9*nhidden).toFloat\n",
    "    model(\"W3\") = FFilter.FFilter2Ddn(10,10,nhidden2,nhidden3,1,0);                     // This is actually an FC layer\n",
    "    model(\"W3\")(?) = normrnd(0,1,10*10*nhidden2*nhidden3,1) / math.sqrt(10*10*nhidden2).toFloat\n",
    "    model(\"W4\") = FFilter.FFilter2Ddn(1,1,nhidden3,nactions,1,0);                       // Second FC layer\n",
    "    model(\"W4\")(?) = normrnd(0,1,nactions*nhidden3,1) / math.sqrt(nhidden3).toFloat\n",
    "    \n",
    "    \n",
    "    for ((k,v) <- model) {\n",
    "        grad.put(k, v.copy);\n",
    "        gradsq.put(k, v.copy);\n",
    "        grad(k).clear\n",
    "        gradsq(k).clear\n",
    "    } \n",
    "\n",
    "    def _forward(s:FND):(FMat, FND, FND, FMat) = {\n",
    "//        \"\"\" Run the model forward given a state as input.\n",
    "//    returns action predictions and the hidden state\"\"\"        \n",
    "        val h = model(\"W1\") * s                  // Forward convolution, h is a tensor\n",
    "        h ~ h *@ (h>=0)                          // ReLU nonlinearity\n",
    "        val h2 = model(\"W2\") * h                 // Forward convolution, h2 is a tensor\n",
    "        h2 ~ h2 *@ (h2 >= 0);\n",
    "        val h3 = model(\"W3\").asMat ^* h2.asMat;  // FC matrix multiply, h3 is a matrix\n",
    "        h3 ~ h3 *@ (h3 >= 0);                   \n",
    "        val rew = model(\"W4\").asMat ^* h3;       // Final FC layer, rew is a matrix\n",
    "        (rew, h, h2, h3)\n",
    "    }\n",
    "    \n",
    "    def forward(s:FMat):(FMat, FND, FND, FMat) = {\n",
    "        val ss = s.asFND(height,width,nwindow,npar).transpose(2,0,1,3);  // Fix input state tensor format\n",
    "        _forward(ss);\n",
    "    }\n",
    "    \n",
    "    \n",
    "    def predict(s:FMat):FMat = {\n",
    "//        \"\"\" Predict the action rewards from a given input state\"\"\"\n",
    "        val (rew, h, h2, h3) = forward(s)\n",
    "        rew\n",
    "    }\n",
    "    \n",
    "    var selector:IMat = irow(0->npar)*nactions\n",
    "              \n",
    "    def gradient(s:FMat, a:IMat, y:FMat):Float = {\n",
    "//        \"\"\" Given a state s, action a and target y, compute the model gradients\"\"\"\n",
    "        val ss = s.asFND(height,width,nwindow,npar).transpose(2,0,1,3);  // Fix the input state tensor format\n",
    "        val (rew, h, h2, h3) = _forward(ss);                             // Run forward\n",
    "        val dout = y - rew(a + selector);                                // Get the residuals for the actions a\n",
    "        val arew = zeros(rew.nrows, rew.ncols);                      \n",
    "        arew(a + selector) = dout                                        // The gradient for the actions a\n",
    "        \n",
    "        grad(\"W4\").asMat ~ grad(\"W4\").asMat + (h3 *^ arew);              // Backward model gradient (FC)\n",
    "        val dh3 = (model(\"W4\").asMat * arew);                            // Backward data gradient (FC)\n",
    "        dh3 ~ dh3 *@ (h3 > 0)                                            // Map gradients through the RELU\n",
    "        \n",
    "        grad(\"W3\").asMat ~ grad(\"W3\").asMat + (h2.asMat *^ dh3);         // Backward model gradient (FC)\n",
    "        val dh2 = (model(\"W3\").asMat * dh3).asFND(nhidden2,10,10,npar);  // Backward data gradient (FC)\n",
    "        dh2 ~ dh2 *@ (h2 > 0)        \n",
    "        \n",
    "//        grad(\"W2\") ~ grad(\"W2\") + (dh2 *^ h);\n",
    "        grad(\"W2\").convolveM(h, dh2, false);                             // Backard model gradient (conv)\n",
    "        val dh = model(\"W2\") ^* dh2                                      // Backward data gradient (conv)\n",
    "        dh ~ dh *@ (h > 0)\n",
    "        \n",
    "//        grad(\"W1\") ~ grad(\"W1\") + (dh *^ s);\n",
    "        grad(\"W1\").convolveM(ss, dh, false);                             // Backward model gradient (conv)\n",
    "\n",
    "        sqrt((dout dotr dout)/dout.length).v                             // return the RMS residual error. \n",
    "    }\n",
    "    \n",
    "    def msprop(learning_rate:Float, decay_rate:Float) = {                // MSprop, i.e. RMSprop without the square root.\n",
    "//        \"\"\" Perform model updates from the gradients using MSprop\"\"\"\n",
    "        for ((k,v) <- model) {\n",
    "            val g = grad(k).data;\n",
    "            val gsq = gradsq(k).data;\n",
    "            val m = model(k).data;\n",
    "            val len = grad(k).length;\n",
    "            var i = 0;\n",
    "            while (i < len) {\n",
    "                val gi = math.min(gclip, math.max(-gclip, g(i)));\n",
    "                gsq(i) = decay_rate * gsq(i) + (1-decay_rate) * gi * gi;\n",
    "                m(i) += learning_rate * gi / (gsq(i) + rmseps);\n",
    "                g(i) = 0;             \n",
    "                i += 1;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement $\\epsilon$-Greedy Policy\n",
    "\n",
    "An $\\epsilon$-Greedy policy should:\n",
    "* with probability $\\epsilon$ take a uniformly-random action.\n",
    "* otherwise choose the best action according to the estimator from the given state.\n",
    "\n",
    "The function below should implement this policy. For each input state, it should return a (column) vector of size nactions which are the probabilities of taking each action. Thus, the probabilities of non-optimal actions should be $\\epsilon/{\\rm nactions}$ and the probability of the best action should be $1-\\epsilon+\\epsilon/{\\rm nactions}$.\n",
    "\n",
    "Since the function processes batches of states, the input <code>state</code> is a <code>nfeatures x nbatch</code> matrix, and the returned value should be a <code>nactions x nbatch</code> matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "val aselector = irow(0->npar)*nactions\n",
    "\n",
    "def policy(estimator:Estimator, state:FMat, epsilon:Float):FMat = {\n",
    "//    \"\"\" Take an estimator and state and predict the best action.\n",
    "//    For each input state, return a vector of action probabilities according to an epsilon-greedy policy\"\"\"\n",
    "    val A = ones(nactions, npar) * (epsilon / nactions)\n",
    "    val q_values = estimator.predict(state)\n",
    "    val (_,best_action) = maxi2(q_values)\n",
    "    A(best_action + aselector) = A(best_action + aselector) + (1f - epsilon)\n",
    "    A\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This routine copies the state of one estimator into another. Its used to update the target estimator from the Q-estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "def update_estimator(to_estimator:Estimator, from_estimator:Estimator, window:Int, istep:Int) = {\n",
    "//    \"\"\" every <window> steps, Copy model state from from_estimator into to_estimator\"\"\"\n",
    "    if (istep % window == 0) {\n",
    "        for ((k,v) <- from_estimator.model) {\n",
    "            to_estimator.model(k) <-- from_estimator.model(k);\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement \"Asynchronous Threads\"\n",
    "\n",
    "Don't try that in Python!! Actually all we do here is create an array of environments and advance each one a random number of steps, using random actions at each step. Later on we will make *synchronous* updates to all the environments, but the environments (and their gradient updates) should remain uncorrelated. This serves the same goal as asynchronous updates in paper [2], or experience replay in paper [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing games...61908 steps, 48 epochs in 75.4340 seconds at 1.2185 msecs/step\n"
     ]
    }
   ],
   "source": [
    "// Create estimators\n",
    "val q_estimator = new Estimator(nhidden, nhidden2, nhidden3, nactions)\n",
    "val target_estimator = new Estimator(nhidden, nhidden2, nhidden3, nactions)\n",
    "\n",
    "// The epsilon and learning rate decay schedules\n",
    "// val epsilons = np.linspace(epsilon_start, epsilon_end, neps)\n",
    "val epsilons = epsilon_start / (1f + row(0->neps)/(neps*epsilon_end/epsilon_start))\n",
    "val learning_rates = learning_rate - row(0 -> nlr) * ((lr_end - learning_rate) / nlr)\n",
    "\n",
    "// Initialize the games\n",
    "print(\"Initializing games...\")\n",
    "val envs = new Array[ALE](npar)\n",
    "val state = zeros(nfeats * nwindow, npar)\n",
    "var total_time=0f\n",
    "var total_steps=0\n",
    "var total_epochs = 0\n",
    "\n",
    "import java.util.Random\n",
    "val rn = new Random\n",
    "\n",
    "tic\n",
    "for (i <- 0 until npar) {\n",
    "    envs(i) = new ALE\n",
    "    envs(i).setInt(\"random_seed\", i)\n",
    "    envs(i).loadROM(game_bin)\n",
    "\n",
    "    val nmoves = rn.nextInt(init_moves - nwindow) + nwindow\n",
    "    for (j <- 0 until nmoves) {   \n",
    "        val action = VALID_ACTIONS(rn.nextInt(nactions))\n",
    "        val (obs, reward, done) = envs(i).step(action)\n",
    "        total_steps += 1;\n",
    "        if (nmoves - j <= nwindow) {\n",
    "            val k = nwindow - nmoves + j;\n",
    "            state((k*nfeats)->((k+1)*nfeats), i) = preprocess(obs)\n",
    "        }\n",
    "        if (done) {\n",
    "            envs(i).reset() \n",
    "            total_epochs += 1\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "total_time = toc     \n",
    "println(\"%d steps, %d epochs in %5.4f seconds at %5.4f msecs/step\" format(\n",
    "    total_steps, total_epochs, total_time, 1000f*total_time/total_steps))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select integer actions using the probability distribution in each column of <code>probs</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "def random_choices(probs:FMat):IMat = {\n",
    "    val result = izeros(1, probs.ncols);\n",
    "    var i = 0;\n",
    "    while (i < probs.ncols) {\n",
    "        val r = rn.nextFloat();\n",
    "        var j = 0;\n",
    "        var cumprob = probs(0, i);\n",
    "        while (r > cumprob && j+1 < probs.length) {\n",
    "            j += 1;\n",
    "            cumprob += probs(j, i);\n",
    "        }\n",
    "        result(i) = j;\n",
    "        i += 1\n",
    "    }\n",
    "    result\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implement Deep Q-Learning\n",
    "In this cell you actually implement the algorithm. We've given you comments to define all the steps. You should also add book-keeping steps to keep track of the loss, reward and number of epochs (where env.step() returns done = true). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, time 0.2, loss 0.00000265, epochs 0, reward/epoch 0.00000, cum reward/epoch 0.00000\n",
      "step 10000, time 414.4, loss 0.06940310, epochs 136, reward/epoch -20.68382, cum reward/epoch -20.68382\n",
      "step 20000, time 832.6, loss 0.05780986, epochs 248, reward/epoch -20.22322, cum reward/epoch -20.47581\n",
      "step 30000, time 1270.2, loss 0.05724451, epochs 354, reward/epoch -19.20755, cum reward/epoch -20.09604\n",
      "step 40000, time 1719.7, loss 0.05864828, epochs 448, reward/epoch -19.57447, cum reward/epoch -19.98661\n",
      "step 50000, time 2159.7, loss 0.05902649, epochs 539, reward/epoch -19.24176, cum reward/epoch -19.86085\n",
      "step 60000, time 2610.8, loss 0.06010906, epochs 627, reward/epoch -19.11364, cum reward/epoch -19.75598\n",
      "step 70000, time 3057.6, loss 0.06163091, epochs 710, reward/epoch -18.55422, cum reward/epoch -19.61549\n",
      "step 80000, time 3516.2, loss 0.06001914, epochs 790, reward/epoch -18.98750, cum reward/epoch -19.55190\n",
      "step 90000, time 3979.6, loss 0.06087767, epochs 872, reward/epoch -18.78049, cum reward/epoch -19.47936\n",
      "step 100000, time 4444.1, loss 0.06093651, epochs 950, reward/epoch -18.29487, cum reward/epoch -19.38210\n",
      "step 110000, time 4917.1, loss 0.06022137, epochs 1029, reward/epoch -18.13924, cum reward/epoch -19.28669\n",
      "step 120000, time 5376.3, loss 0.06087659, epochs 1100, reward/epoch -19.29577, cum reward/epoch -19.28727\n",
      "step 130000, time 5828.2, loss 0.05915862, epochs 1174, reward/epoch -18.54054, cum reward/epoch -19.24020\n",
      "step 140000, time 6276.4, loss 0.06170001, epochs 1247, reward/epoch -17.41096, cum reward/epoch -19.13312\n",
      "step 150000, time 6732.6, loss 0.05969787, epochs 1316, reward/epoch -19.50725, cum reward/epoch -19.15273\n",
      "step 160000, time 7176.7, loss 0.06211072, epochs 1387, reward/epoch -17.92958, cum reward/epoch -19.09012\n",
      "step 170000, time 7625.8, loss 0.06181044, epochs 1457, reward/epoch -17.38571, cum reward/epoch -19.00824\n",
      "step 180000, time 8074.8, loss 0.06002636, epochs 1522, reward/epoch -18.53846, cum reward/epoch -18.98817\n",
      "step 190000, time 8532.5, loss 0.06158220, epochs 1589, reward/epoch -17.95522, cum reward/epoch -18.94462\n",
      "step 200000, time 8995.9, loss 0.06081722, epochs 1653, reward/epoch -19.10938, cum reward/epoch -18.95100\n",
      "step 210000, time 9447.9, loss 0.06176099, epochs 1723, reward/epoch -17.71428, cum reward/epoch -18.90075\n",
      "step 220000, time 9913.1, loss 0.06420839, epochs 1794, reward/epoch -16.77465, cum reward/epoch -18.81661\n",
      "step 230000, time 10374.4, loss 0.06125272, epochs 1861, reward/epoch -18.79104, cum reward/epoch -18.81569\n",
      "step 240000, time 10843.1, loss 0.06231732, epochs 1931, reward/epoch -17.37143, cum reward/epoch -18.76333\n",
      "step 250000, time 11299.4, loss 0.06088262, epochs 1997, reward/epoch -18.50000, cum reward/epoch -18.75463\n",
      "step 260000, time 11771.3, loss 0.06081666, epochs 2062, reward/epoch -18.83077, cum reward/epoch -18.75703\n",
      "step 270000, time 12232.2, loss 0.06185173, epochs 2130, reward/epoch -17.77941, cum reward/epoch -18.72582\n",
      "step 280000, time 12688.3, loss 0.06183449, epochs 2194, reward/epoch -18.45313, cum reward/epoch -18.71787\n",
      "step 290000, time 13144.2, loss 0.06242165, epochs 2263, reward/epoch -16.91304, cum reward/epoch -18.66284\n",
      "step 300000, time 13604.6, loss 0.06280141, epochs 2327, reward/epoch -18.84375, cum reward/epoch -18.66781\n",
      "step 310000, time 14074.6, loss 0.06133507, epochs 2393, reward/epoch -17.96970, cum reward/epoch -18.64856\n",
      "step 320000, time 14546.1, loss 0.06093457, epochs 2457, reward/epoch -17.53125, cum reward/epoch -18.61946\n",
      "step 330000, time 15012.9, loss 0.06058913, epochs 2522, reward/epoch -17.78461, cum reward/epoch -18.59794\n",
      "step 340000, time 15478.2, loss 0.06218781, epochs 2587, reward/epoch -17.87692, cum reward/epoch -18.57982\n",
      "step 350000, time 15945.2, loss 0.06381033, epochs 2652, reward/epoch -17.87692, cum reward/epoch -18.56259\n",
      "step 360000, time 16415.8, loss 0.06346992, epochs 2717, reward/epoch -17.29231, cum reward/epoch -18.53221\n",
      "step 370000, time 16877.3, loss 0.06132525, epochs 2781, reward/epoch -17.50000, cum reward/epoch -18.50845\n",
      "step 380000, time 17340.9, loss 0.06273377, epochs 2843, reward/epoch -18.16129, cum reward/epoch -18.50088\n",
      "step 390000, time 17812.5, loss 0.06229503, epochs 2903, reward/epoch -16.51667, cum reward/epoch -18.45987\n",
      "step 400000, time 18292.4, loss 0.06294511, epochs 2969, reward/epoch -17.24242, cum reward/epoch -18.43281\n",
      "step 410000, time 18765.3, loss 0.06242180, epochs 3026, reward/epoch -18.01754, cum reward/epoch -18.42498\n",
      "step 420000, time 19227.0, loss 0.06162761, epochs 3088, reward/epoch -17.27419, cum reward/epoch -18.40188\n",
      "step 430000, time 19690.7, loss 0.06166721, epochs 3150, reward/epoch -17.40322, cum reward/epoch -18.38222\n",
      "step 440000, time 20157.0, loss 0.06156560, epochs 3210, reward/epoch -17.08333, cum reward/epoch -18.35794\n",
      "step 450000, time 20619.6, loss 0.06198762, epochs 3270, reward/epoch -16.83333, cum reward/epoch -18.32997\n",
      "step 460000, time 21087.5, loss 0.06221019, epochs 3329, reward/epoch -17.45763, cum reward/epoch -18.31451\n",
      "step 470000, time 21554.1, loss 0.06091767, epochs 3388, reward/epoch -18.93220, cum reward/epoch -18.32527\n",
      "step 480000, time 22022.3, loss 0.06342365, epochs 3453, reward/epoch -16.50769, cum reward/epoch -18.29105\n",
      "step 490000, time 22496.8, loss 0.06165421, epochs 3513, reward/epoch -17.96667, cum reward/epoch -18.28551\n",
      "step 500000, time 22976.8, loss 0.06249196, epochs 3574, reward/epoch -17.08197, cum reward/epoch -18.26497\n",
      "step 510000, time 23450.6, loss 0.06035426, epochs 3634, reward/epoch -17.18333, cum reward/epoch -18.24711\n",
      "step 520000, time 23922.8, loss 0.06168210, epochs 3692, reward/epoch -17.20690, cum reward/epoch -18.23077\n",
      "step 530000, time 24414.9, loss 0.06218327, epochs 3751, reward/epoch -18.01695, cum reward/epoch -18.22741\n",
      "step 540000, time 24884.1, loss 0.06302397, epochs 3809, reward/epoch -16.72414, cum reward/epoch -18.20452\n",
      "step 550000, time 25349.0, loss 0.06330004, epochs 3867, reward/epoch -16.81034, cum reward/epoch -18.18361\n",
      "step 560000, time 25825.2, loss 0.06175981, epochs 3922, reward/epoch -17.18182, cum reward/epoch -18.16956\n",
      "step 570000, time 26297.5, loss 0.06215604, epochs 3984, reward/epoch -16.41936, cum reward/epoch -18.14232\n",
      "step 580000, time 26763.6, loss 0.06110997, epochs 4038, reward/epoch -17.46296, cum reward/epoch -18.13323\n",
      "step 590000, time 27235.7, loss 0.06210413, epochs 4097, reward/epoch -16.27119, cum reward/epoch -18.10642\n",
      "step 600000, time 27703.9, loss 0.06161884, epochs 4155, reward/epoch -17.63793, cum reward/epoch -18.09988\n",
      "step 610000, time 28180.8, loss 0.06242161, epochs 4209, reward/epoch -17.55556, cum reward/epoch -18.09290\n",
      "step 620000, time 28657.2, loss 0.06106909, epochs 4267, reward/epoch -17.06897, cum reward/epoch -18.07898\n",
      "step 630000, time 29123.6, loss 0.06227521, epochs 4327, reward/epoch -16.91667, cum reward/epoch -18.06286\n",
      "step 640000, time 29586.9, loss 0.06171867, epochs 4383, reward/epoch -16.96428, cum reward/epoch -18.04882\n",
      "step 650000, time 30066.1, loss 0.06136654, epochs 4440, reward/epoch -16.85965, cum reward/epoch -18.03356\n",
      "step 660000, time 30542.8, loss 0.06183797, epochs 4498, reward/epoch -17.62069, cum reward/epoch -18.02823\n",
      "step 670000, time 31026.9, loss 0.06217204, epochs 4556, reward/epoch -17.13793, cum reward/epoch -18.01690\n",
      "step 680000, time 31517.1, loss 0.06115621, epochs 4610, reward/epoch -17.85185, cum reward/epoch -18.01497\n",
      "step 690000, time 31999.9, loss 0.06156588, epochs 4669, reward/epoch -16.05085, cum reward/epoch -17.99015\n",
      "step 700000, time 32472.7, loss 0.06165743, epochs 4722, reward/epoch -17.81132, cum reward/epoch -17.98814\n",
      "step 710000, time 32948.1, loss 0.05886113, epochs 4778, reward/epoch -17.16072, cum reward/epoch -17.97844\n",
      "step 720000, time 33416.0, loss 0.06175082, epochs 4834, reward/epoch -17.48214, cum reward/epoch -17.97269\n",
      "step 730000, time 33891.0, loss 0.06160389, epochs 4889, reward/epoch -17.07273, cum reward/epoch -17.96257\n",
      "step 740000, time 34364.3, loss 0.06022770, epochs 4948, reward/epoch -17.23729, cum reward/epoch -17.95392\n",
      "step 750000, time 34834.8, loss 0.06176427, epochs 5002, reward/epoch -17.18518, cum reward/epoch -17.94562\n",
      "step 760000, time 35305.9, loss 0.06250466, epochs 5062, reward/epoch -17.26667, cum reward/epoch -17.93757\n",
      "step 770000, time 35784.9, loss 0.06096103, epochs 5121, reward/epoch -16.27119, cum reward/epoch -17.91838\n",
      "step 780000, time 36267.0, loss 0.06124752, epochs 5176, reward/epoch -17.58182, cum reward/epoch -17.91480\n",
      "step 790000, time 36746.9, loss 0.05987538, epochs 5227, reward/epoch -17.03922, cum reward/epoch -17.90626\n",
      "step 800000, time 37220.5, loss 0.06116328, epochs 5285, reward/epoch -17.68966, cum reward/epoch -17.90388\n",
      "step 810000, time 37695.8, loss 0.06098319, epochs 5340, reward/epoch -17.03636, cum reward/epoch -17.89494\n",
      "step 820000, time 38182.4, loss 0.06066382, epochs 5394, reward/epoch -16.85185, cum reward/epoch -17.88450\n",
      "step 830000, time 38662.6, loss 0.06274740, epochs 5451, reward/epoch -16.70175, cum reward/epoch -17.87213\n",
      "step 840000, time 39138.6, loss 0.06007649, epochs 5504, reward/epoch -16.90566, cum reward/epoch -17.86283\n",
      "step 850000, time 39616.0, loss 0.06024880, epochs 5557, reward/epoch -16.13208, cum reward/epoch -17.84632\n",
      "step 860000, time 40084.6, loss 0.06101366, epochs 5609, reward/epoch -17.07692, cum reward/epoch -17.83919\n",
      "step 870000, time 40561.0, loss 0.06079803, epochs 5663, reward/epoch -17.22222, cum reward/epoch -17.83330\n",
      "step 880000, time 41031.4, loss 0.06033181, epochs 5718, reward/epoch -17.09091, cum reward/epoch -17.82616\n",
      "step 890000, time 41508.1, loss 0.06007029, epochs 5771, reward/epoch -17.75472, cum reward/epoch -17.82551\n",
      "step 900000, time 41980.1, loss 0.06112753, epochs 5824, reward/epoch -16.56604, cum reward/epoch -17.81404\n",
      "step 910000, time 42444.7, loss 0.06136134, epochs 5878, reward/epoch -16.53704, cum reward/epoch -17.80231\n",
      "step 920000, time 42922.7, loss 0.06064395, epochs 5932, reward/epoch -15.96296, cum reward/epoch -17.78557\n",
      "step 930000, time 43399.0, loss 0.06029835, epochs 5986, reward/epoch -15.62963, cum reward/epoch -17.76612\n",
      "step 940000, time 43879.7, loss 0.05973431, epochs 6033, reward/epoch -18.36170, cum reward/epoch -17.77076\n",
      "step 950000, time 44350.6, loss 0.05954161, epochs 6086, reward/epoch -16.47170, cum reward/epoch -17.75945\n",
      "step 960000, time 44816.7, loss 0.05881334, epochs 6140, reward/epoch -16.81482, cum reward/epoch -17.75114\n",
      "step 970000, time 45296.4, loss 0.06036895, epochs 6194, reward/epoch -16.51852, cum reward/epoch -17.74039\n",
      "step 980000, time 45795.7, loss 0.06074988, epochs 6244, reward/epoch -16.78000, cum reward/epoch -17.73270\n",
      "step 990000, time 46498.6, loss 0.06149291, epochs 6296, reward/epoch -16.78846, cum reward/epoch -17.72491\n",
      "step 1000000, time 47268.2, loss 0.06091230, epochs 6348, reward/epoch -16.71154, cum reward/epoch -17.71660\n",
      "step 1010000, time 48034.1, loss 0.06074350, epochs 6403, reward/epoch -16.74545, cum reward/epoch -17.70826\n",
      "step 1020000, time 48798.3, loss 0.05990763, epochs 6453, reward/epoch -16.26000, cum reward/epoch -17.69704\n",
      "step 1030000, time 49565.9, loss 0.05914222, epochs 6504, reward/epoch -16.35294, cum reward/epoch -17.68650\n",
      "step 1040000, time 50332.8, loss 0.06102899, epochs 6557, reward/epoch -16.20755, cum reward/epoch -17.67455\n",
      "step 1050000, time 51108.9, loss 0.05907011, epochs 6606, reward/epoch -17.42857, cum reward/epoch -17.67272\n",
      "step 1060000, time 51876.8, loss 0.05857850, epochs 6657, reward/epoch -17.09804, cum reward/epoch -17.66832\n",
      "step 1070000, time 52643.8, loss 0.05849670, epochs 6709, reward/epoch -16.80769, cum reward/epoch -17.66165\n",
      "step 1080000, time 53411.1, loss 0.05993674, epochs 6763, reward/epoch -17.22222, cum reward/epoch -17.65814\n",
      "step 1090000, time 54179.7, loss 0.06012505, epochs 6813, reward/epoch -16.00000, cum reward/epoch -17.64597\n",
      "step 1100000, time 54960.2, loss 0.05914338, epochs 6866, reward/epoch -17.18868, cum reward/epoch -17.64244\n",
      "step 1110000, time 55737.2, loss 0.06016887, epochs 6915, reward/epoch -17.24490, cum reward/epoch -17.63962\n",
      "step 1120000, time 56500.8, loss 0.05872529, epochs 6965, reward/epoch -16.24000, cum reward/epoch -17.62958\n"
     ]
    }
   ],
   "source": [
    "tic\n",
    "var block_loss = 0f\n",
    "var block_reward = 0f\n",
    "var total_reward = 0f\n",
    "total_epochs = 0\n",
    "var last_epochs = 0\n",
    "val new_state = state.copy\n",
    "\n",
    "val times = zeros(1,8)\n",
    "val dtimes = zeros(1,7)\n",
    "for (istep <- 0 until nsteps) {\n",
    "//    if (render): envs[0].render()\n",
    "    times(0) = toc\n",
    "    val epsilon = epsilons(math.min(istep, neps-1));                                // get an epsilon for the eps-greedy policy\n",
    "    val lr = learning_rates(math.min(istep, nlr-1));                                // update the decayed learning rate\n",
    "    \n",
    "    update_estimator(target_estimator, q_estimator, target_window, istep);          // update the target estimator if needed    \n",
    "    times(1) = toc\n",
    "    \n",
    "    val action_probs = policy(q_estimator, state, epsilon);                         // get the next action probabilities from the policy\n",
    "    times(2) = toc\n",
    "                                                          \n",
    "    val actions = random_choices(action_probs);                                     // Choose actions using the policy\n",
    "    val (obs, rewards, dones) = ALE.stepAll2(envs, VALID_ACTIONS(actions))           // step through parallel envs\n",
    "    times(3) = toc\n",
    "    \n",
    "    for (i <- 0 until npar) {                                                     \n",
    "        val img = preprocess(obs(i));                                               // process the observation\n",
    "        new_state(?,i) = state(nfeats->state.nrows,i) on img;                       // add it to buffer of last nwindow imgs        \n",
    "    }    \n",
    "    total_epochs += sum(dones).v.toInt\n",
    "    block_reward += sum(rewards).v  \n",
    "    times(4) = toc\n",
    "    \n",
    "    val q_values_next = target_estimator.predict(new_state);                        // predict the Q values\n",
    "    times(5) = toc\n",
    "    \n",
    "    dones <-- (dones + (rewards != 0f) > 0f);\n",
    "    val targets = rewards+discount_factor*(1f-dones) *@ maxi(q_values_next);        // compute target values   \n",
    "    block_loss += q_estimator.gradient(state, actions, targets);                    // compute q-estimator gradient and return the loss\n",
    "    times(6) = toc\n",
    "    \n",
    "    if (istep % rmsevery == 0) {\n",
    "        q_estimator.msprop(lr, gsq_decay);                       // apply the gradient update\n",
    "//        print(\"ds1=%f, ss1=%f, ds2=%f, ss2=%f\\n\" format(res(0,0), res(1,0), res(0,1), res(1,1)));\n",
    "    }\n",
    "    times(7) = toc\n",
    "    \n",
    "    dtimes ~ dtimes + (times(0,1->8) - times(0,0->7))\n",
    "    val t = toc;\n",
    "    if (istep % printsteps == 0) {\n",
    "        total_reward += block_reward;\n",
    "        println(\"step %d, time %2.1f, loss %9.8f, epochs %d, reward/epoch %6.5f, cum reward/epoch %6.5f\" format(\n",
    "                istep, t, block_loss/printsteps, total_epochs, block_reward/math.max(1,total_epochs-last_epochs), total_reward/math.max(1,total_epochs)))\n",
    "        last_epochs = total_epochs;\n",
    "        block_reward = 0f;\n",
    "        block_loss = 0f;\n",
    "    }\n",
    "    state <-- new_state;\n",
    "}\n",
    "dtimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And animate the model's performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val layer=\"W1\"\n",
    "q_estimator.model(layer).dims on q_estimator.grad(layer).dims \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "/*state0 = state[:,0]\n",
    "for i in np.arange(200):\n",
    "    envs[0].render()\n",
    "    preds = test_estimator.predict(state0)\n",
    "    iaction = np.argmax(preds)\n",
    "    obs, _, done0, _ = envs[0].step(VALID_ACTIONS[iaction])\n",
    "    state0 = np.concatenate((state0[nfeats:], preprocess(obs)))\n",
    "    if (done0): envs[0].reset() */\n",
    "//hist(ln(abs(q_estimator.gradsq(\"W1\")(?))),100)\n",
    "//    hist(ln(abs(q_estimator.gradsq(\"W1\")(?))),100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there we have it. Simple 1-step Q-Learning can solve easy problems very fast. Note that environments that produce images will be much slower to train on than environments (like CartPole) which return an observation of the state of the system. But this model can still train on those image-based games - like Atari games. It will take hours-days however. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//q_estimator.gradient(state,actions,targets)\n",
    "q_estimator.model(\"W2\").dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val f=FFilter.FFilter2Ddn(7,7,4,16,4,3);\n",
    "f.dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val test_steps = 100001;\n",
    "val testprintsteps = 10000;\n",
    "\n",
    "val test_estimator = new Estimator(nfeats*nwindow, nhidden, nactions);\n",
    "test_estimator.model(\"W1\") = loadFMat(\"BestW1.fmat.lz4\");\n",
    "test_estimator.model(\"W2\") = loadFMat(\"BestW2.fmat.lz4\");\n",
    "\n",
    "block_reward = 0f;\n",
    "total_reward = 0f;\n",
    "total_epochs = 0;\n",
    "last_epochs = 0;\n",
    "\n",
    "tic;\n",
    "for (istep <- 0 until test_steps) {\n",
    "    \n",
    "    val action_probs = policy(test_estimator, state, 0);                    // get the next action probabilities from the policy                                                         \n",
    "    val actions = random_choices(action_probs);                             // Choose actions using the policy\n",
    "    val (obs, rewards, dones) = ALE.stepAll2(envs, VALID_ACTIONS(actions))  // step through parallel envs   \n",
    "    for (i <- 0 until npar) {                                                     \n",
    "        val img = preprocess(obs(i));                                       // process the observation\n",
    "        state(?,i) = state(nfeats->state.nrows,i) on img;                   // add it to buffer of last nwindow imgs        \n",
    "    }    \n",
    "    total_epochs += sum(dones).v.toInt\n",
    "    block_reward += sum(rewards).v  \n",
    "    \n",
    "    val t = toc;\n",
    "    if (istep % testprintsteps == 0) {\n",
    "        total_reward += block_reward;\n",
    "        println(\"step %d, time %2.1f, epochs %d, reward/epoch %6.5f, cum reward/epoch %6.5f\" format(\n",
    "                istep, t, total_epochs, block_reward/math.max(1,total_epochs-last_epochs), total_reward/math.max(1,total_epochs)))\n",
    "        last_epochs = total_epochs;\n",
    "        block_reward = 0f;\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "name": "scala",
   "version": "2.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
