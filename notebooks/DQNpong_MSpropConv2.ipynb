{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning\n",
    "\n",
    "In this notebook, you will implement a deep Q-Learning reinforcement algorithm. The implementation borrows ideas from both the original DeepMind Nature paper and the more recent asynchronous version:<br/>\n",
    "[1] \"Human-Level Control through Deep Reinforcement Learning\" by Mnih et al. 2015<br/>\n",
    "[2] \"Asynchronous Methods for Deep Reinforcement Learning\" by Mnih et al. 2016.<br/>\n",
    "\n",
    "In particular:\n",
    "* We use separate target and Q-functions estimators with periodic updates to the target estimator. \n",
    "* We use several concurrent \"threads\" rather than experience replay to generate less biased gradient updates. \n",
    "* Threads are actually synchronized so we start each one at a random number of moves.\n",
    "* We use an epsilon-greedy policy that blends random moves with policy moves.\n",
    "* We taper the random search parameter (epsilon) and the learning rate to zero during training.\n",
    "\n",
    "This gives a simple and reasonably fast general-purpose RL algorithm. We use it here for the Cartpole environment from OpenAI Gym, but it can easily be adapted to others. For this notebook, you will implement 4 steps:\n",
    "\n",
    "1. The backward step for the Q-estimator\n",
    "2. The $\\epsilon$-greedy policy\n",
    "3. \"asynchronous\" initialization \n",
    "4. The Q-learning algorithm\n",
    "\n",
    "To get started, we import some prerequisites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 CUDA device found, CUDA version 8.0\n"
     ]
    }
   ],
   "source": [
    "import BIDMat.{CMat,CSMat,DMat,Dict,FMat,FFilter,Filter,GFilter,GMat,GDMat,GIMat,GLMat,GSMat,GSDMat,HMat,IDict,Image,IMat,LMat,Mat,SMat,SBMat,SDMat,TMat}\n",
    "import BIDMat.MatFunctions._\n",
    "import BIDMat.SciFunctions._\n",
    "import BIDMat.Solvers._\n",
    "import BIDMat.JPlotting._\n",
    "import BIDMach.Learner\n",
    "\n",
    "import BIDMach.models.{Click,FM,GLM,KMeans,KMeansw,LDA,LDAgibbs,Model,NMF,SFA,RandomForest,SVD}\n",
    "import BIDMach.networks.{Net}\n",
    "import BIDMach.datasources.{DataSource,MatSource,FileSource,SFileSource}\n",
    "import BIDMach.datasinks.{DataSink,MatSink}\n",
    "import BIDMach.mixins.{CosineSim,Perplexity,Top,L1Regularizer,L2Regularizer}\n",
    "import BIDMach.updaters.{ADAGrad,Batch,BatchNorm,Grad,IncMult,IncNorm,Telescoping}\n",
    "import BIDMach.causal.{IPTW}\n",
    "import BIDMach.rl.ALE\n",
    "\n",
    "Mat.checkMKL(false)\n",
    "Mat.checkCUDA\n",
    "Mat.setInline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ale = new ALE\n",
    "ale.getFloat(\"repeat_action_probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below lists some parameters you can tune. They should be self-explanatory. They are currently set to train CartPole-V0 to a \"solved\" score (> 195) most of the time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nsteps = 20000001                    // Number of steps to run (game actions per environment)\n",
    "val npar = 64                            // Number of parallel environments\n",
    "val target_window = 20                  // Interval to update target estimator from q-estimator\n",
    "val discount_factor = 0.99f              // Reward discount factor\n",
    "val printsteps = 10000                   // Number of steps between printouts\n",
    "val render = false                       // Whether to render an environment while training\n",
    "\n",
    "val epsilon_start = 0.5f                 // Parameters for epsilon-greedy policy: initial epsilon\n",
    "val epsilon_end = 0.1f                  // Final epsilon\n",
    "val neps = (0.9*nsteps).toInt            // Number of steps to decay epsilon\n",
    "\n",
    "//val learning_rate = 5e-4f\n",
    "val learning_rate = 1e-5f                // Initial learning rate\n",
    "val lr_end = learning_rate               // Final learning rate\n",
    "val nlr = neps                           // Steps to decay learning rate\n",
    "val gsq_decay = 0.99f                   // Decay factor for RMSProp\n",
    "val momentum_decay = 0.9f\n",
    "val gclip = 1f\n",
    "val rmseps = 1e-5f\n",
    "val rmsevery = 2\n",
    "\n",
    "val nhidden = 16                         // Number of hidden layers for estimators\n",
    "val nhidden2 = 32\n",
    "val nhidden3 = 256\n",
    "\n",
    "val init_moves = 10000                   // Upper bound on random number of moves to take initially\n",
    "val nwindow = 4                          // Sensing window = last n images in a state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are environment-specific parameters. The function \"preprocess\" should process an observation returned by the environment into a vector for training. For CartPole we simply append a 1 to implement bias in the first layer. \n",
    "\n",
    "For visual environments you would typically crop, downsample to 80x80, set color to a single bit (foreground/background), and flatten to a vector. \n",
    "\n",
    "*nfeats* is the dimension of the vector output by *preprocess*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "val game_bin=\"/code/ALE/roms/Pong.bin\"                 // Model type and action definitions\n",
    "val VALID_ACTIONS = irow(0, 3, 4)\n",
    "val nactions= VALID_ACTIONS.length\n",
    "val nfeats = 80*80  \n",
    "val height = 80\n",
    "val width = 80\n",
    "\n",
    "def preprocess(I:FMat):FMat = {\n",
    "//  Preprocess Pong game frames into vectors.\n",
    "//  Input:\n",
    "//    - (3,160,210) uint8 frame representing Pong game screen.\n",
    "//  Returns:\n",
    "//    - Downsampled (DxD) matrix of 0s and 1s, \"raveled\" into a 1-D vector.\n",
    "    var i = 0;\n",
    "    val res = zeros(80*80,1)\n",
    "    while (i < 80) {\n",
    "        var j = 0;\n",
    "        while (j < 80) {\n",
    "            val x = I.data(3*(j*2 + 160 * (i*2 + 35)));\n",
    "            res.data(j + 80*i) = {if (x == 144f || x == 109f) 0f else {if (x != 0f) 1f else 0f}};\n",
    "            j += 1;\n",
    "        }\n",
    "        i += 1;\n",
    "    }\n",
    "    res\n",
    "}\n",
    "\n",
    "def preprocess(I:Array[Byte]):FMat = {\n",
    "//  Preprocess Pong game frames into vectors.\n",
    "//  Input:\n",
    "//    - (3,160,210) uint8 frame representing Pong game screen.\n",
    "//  Returns:\n",
    "//    - Downsampled (DxD) matrix of 0s and 1s, \"raveled\" into a 1-D vector.\n",
    "    var i = 0;\n",
    "    val res = zeros(80*80,1)\n",
    "    while (i < 80) {\n",
    "        var j = 0;\n",
    "        while (j < 80) {\n",
    "            val x = I(j*2 + 160 * (i*2 + 35));\n",
    "            res.data(j + 80*i) = {if (x == 34) 0f else {if (x != 0) 1f else 0f}};\n",
    "            j += 1;\n",
    "        }\n",
    "        i += 1;\n",
    "    }\n",
    "    res\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the Q-estimator class. We use two instances of this class, one for the target estimator, and one for the Q-estimator. The Q function is normally represented as a scalar $Q(x,a)$ where $x$ is the state and $a$ is an action. For ease of implementation, we actually estimate a vector-valued function $Q(x,.)$ which returns the estimated reward for every action. The model here has just a single hidden layer:\n",
    "\n",
    "<pre>\n",
    "Input Layer (nfeats) => FC Layer => RELU => FC Layer => Output (naction values)\n",
    "</pre>\n",
    "\n",
    "## 1. Implement Q-estimator gradient\n",
    "Your first task is to implement the\n",
    "<pre>Estimator.gradient(s, a, y)</pre>\n",
    "method for this class. **gradient** should compute the gradients wrt weight arrays W1 and W2 into\n",
    "<pre>self.grad['W1']\n",
    "self.grad['W2']</pre>\n",
    "respectively. Both <code>a</code> and <code>y</code> are vectors. Be sure to update only the output layer weights corresponding to the given action vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import scala.collection.mutable.HashMap;\n",
    "\n",
    "class Estimator(nhidden:Int, nhidden2:Int, nhidden3:Int, nactions:Int) {\n",
    "\n",
    "//        \"\"\" Create model matrices, and gradient and squared gradient buffers\"\"\"\n",
    "    val model = new HashMap[String, FFilter]();\n",
    "    val grad = new HashMap[String, FFilter]();\n",
    "    val gradsq = new HashMap[String, FFilter]();\n",
    "    model(\"W1\") = FFilter.FFilter2Ddn(7,7,nwindow,nhidden,4,3);\n",
    "    model(\"W1\")(?) = normrnd(0,1,49*nwindow*nhidden,1) / math.sqrt(49*nwindow).toFloat  // \"Xavier\" initialization\n",
    "    model(\"W2\") = FFilter.FFilter2Ddn(3,3,nhidden,nhidden2,2,1);\n",
    "    model(\"W2\")(?) = normrnd(0,1,9*nhidden*nhidden2,1) / math.sqrt(9*nhidden).toFloat\n",
    "    model(\"W3\") = FFilter(irow(10,10,nhidden2,nhidden3), iones(1,4), izeros(1,4))\n",
    "    model(\"W3\")(?) = normrnd(0,1,10*10*nhidden2*nhidden3,1) / math.sqrt(10*10*nhidden2).toFloat\n",
    "    model(\"W4\") = FFilter(irow(1,1,nhidden3,nactions), iones(1,4), izeros(1,4))\n",
    "    model(\"W4\")(?) = normrnd(0,1,nactions*nhidden3,1) / math.sqrt(nhidden3).toFloat\n",
    "    \n",
    "    \n",
    "    for ((k,v) <- model) {\n",
    "        grad.put(k, v.copy);\n",
    "        gradsq.put(k, v.copy);\n",
    "        grad(k).clear\n",
    "        gradsq(k).clear\n",
    "    } \n",
    "\n",
    "    def _forward(s:FMat):(FMat, FMat, FMat, FMat) = {\n",
    "//        \"\"\" Run the model forward given a state as input.\n",
    "//    returns action predictions and the hidden state\"\"\"        \n",
    "        val h = model(\"W1\") * s\n",
    "        h ~ h *@ (h>=0)    // ReLU nonlinearity\n",
    "        val h2 = model(\"W2\") * h\n",
    "        h2 ~ h2 *@ (h2 >= 0);\n",
    "        val h3 = model(\"W3\").asMat ^* h2.asMat;\n",
    "        h3 ~ h3 *@ (h3 >= 0);\n",
    "        val rew = model(\"W4\").asMat ^* h3;\n",
    "        (rew, h, h2, h3)\n",
    "    }\n",
    "    \n",
    "    def forward(s:FMat):(FMat, FMat, FMat, FMat) = {\n",
    "        val ss = s.asFMat(height,width,nwindow,npar).transpose(2,0,1,3);\n",
    "        _forward(ss);\n",
    "    }\n",
    "    \n",
    "    \n",
    "    def predict(s:FMat):FMat = {\n",
    "//        \"\"\" Predict the action rewards from a given input state\"\"\"\n",
    "        val (rew, h, h2, h3) = forward(s)\n",
    "        rew\n",
    "    }\n",
    "    \n",
    "    var selector:IMat = irow(0->npar)*nactions\n",
    "              \n",
    "    def gradient(s:FMat, a:IMat, y:FMat):Float = {\n",
    "//        \"\"\" Given a state s, action a and target y, compute the model gradients\"\"\"\n",
    "        val ss = s.asFMat(height,width,nwindow,npar).transpose(2,0,1,3);\n",
    "        val (rew, h, h2, h3) = _forward(ss);\n",
    "        val dout = y - rew(a + selector);     \n",
    "        val arew = zeros(rew.nrows, rew.ncols);\n",
    "        arew(a + selector) = dout\n",
    "        \n",
    "        grad(\"W4\").asMat ~ grad(\"W4\").asMat + (h3 *^ arew);\n",
    "        val dh3 = (model(\"W4\").asMat * arew);\n",
    "        dh3 ~ dh3 *@ (h3 > 0)\n",
    "        \n",
    "        grad(\"W3\").asMat ~ grad(\"W3\").asMat + (h2.asMat *^ dh3);\n",
    "        val dh2 = (model(\"W3\").asMat * dh3).asFMat(nhidden2,10,10,npar);\n",
    "        dh2 ~ dh2 *@ (h2 > 0)        \n",
    "        \n",
    "//        grad(\"W2\") ~ grad(\"W2\") + (dh2 *^ h);\n",
    "        grad(\"W2\").convolveM(h, dh2, false);\n",
    "        val dh = model(\"W2\") ^* dh2\n",
    "        dh ~ dh *@ (h > 0)\n",
    "        \n",
    "//        grad(\"W1\") ~ grad(\"W1\") + (dh *^ s);\n",
    "        grad(\"W1\").convolveM(ss, dh, false);\n",
    "\n",
    "        sqrt((dout dotr dout)/dout.length).v\n",
    "    }\n",
    "    \n",
    "    def msprop(learning_rate:Float, decay_rate:Float) = {\n",
    "//        \"\"\" Perform model updates from the gradients using RMSprop\"\"\"\n",
    "        for ((k,v) <- model) {\n",
    "            val g = grad(k).data;\n",
    "            val gsq = gradsq(k).data;\n",
    "            val m = model(k).data;\n",
    "            val len = grad(k).length;\n",
    "            var i = 0;\n",
    "            while (i < len) {\n",
    "                val gi = math.min(gclip, math.max(-gclip, g(i)));\n",
    "                gsq(i) = decay_rate * gsq(i) + (1-decay_rate) * gi * gi;\n",
    "                m(i) += learning_rate * gi / (gsq(i) + rmseps);\n",
    "                g(i) = 0;             \n",
    "                i += 1;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement $\\epsilon$-Greedy Policy\n",
    "\n",
    "An $\\epsilon$-Greedy policy should:\n",
    "* with probability $\\epsilon$ take a uniformly-random action.\n",
    "* otherwise choose the best action according to the estimator from the given state.\n",
    "\n",
    "The function below should implement this policy. For each input state, it should return a (column) vector of size nactions which are the probabilities of taking each action. Thus, the probabilities of non-optimal actions should be $\\epsilon/{\\rm nactions}$ and the probability of the best action should be $1-\\epsilon+\\epsilon/{\\rm nactions}$.\n",
    "\n",
    "Since the function processes batches of states, the input <code>state</code> is a <code>nfeatures x nbatch</code> matrix, and the returned value should be a <code>nactions x nbatch</code> matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "val aselector = irow(0->npar)*nactions\n",
    "\n",
    "def policy(estimator:Estimator, state:FMat, epsilon:Float):FMat = {\n",
    "//    \"\"\" Take an estimator and state and predict the best action.\n",
    "//    For each input state, return a vector of action probabilities according to an epsilon-greedy policy\"\"\"\n",
    "    val A = ones(nactions, npar) * (epsilon / nactions)\n",
    "    val q_values = estimator.predict(state)\n",
    "    val (_,best_action) = maxi2(q_values)\n",
    "    A(best_action + aselector) = A(best_action + aselector) + (1f - epsilon)\n",
    "    A\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This routine copies the state of one estimator into another. Its used to update the target estimator from the Q-estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "def update_estimator(to_estimator:Estimator, from_estimator:Estimator, window:Int, istep:Int) = {\n",
    "//    \"\"\" every <window> steps, Copy model state from from_estimator into to_estimator\"\"\"\n",
    "    if (istep % window == 0) {\n",
    "        for ((k,v) <- from_estimator.model) {\n",
    "            to_estimator.model(k) <-- from_estimator.model(k);\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement \"Asynchronous Threads\"\n",
    "\n",
    "Don't try that in Python!! Actually all we do here is create an array of environments and advance each one a random number of steps, using random actions at each step. Later on we will make *synchronous* updates to all the environments, but the environments (and their gradient updates) should remain uncorrelated. This serves the same goal as asynchronous updates in paper [2], or experience replay in paper [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing games...317870 steps, 254 epochs in 449.8810 seconds at 1.4153 msecs/step\n"
     ]
    }
   ],
   "source": [
    "// Create estimators\n",
    "val q_estimator = new Estimator(nhidden, nhidden2, nhidden3, nactions)\n",
    "val target_estimator = new Estimator(nhidden, nhidden2, nhidden3, nactions)\n",
    "\n",
    "// The epsilon and learning rate decay schedules\n",
    "// val epsilons = np.linspace(epsilon_start, epsilon_end, neps)\n",
    "val epsilons = epsilon_start / (1f + row(0->neps)/(neps*epsilon_end/epsilon_start))\n",
    "val learning_rates = learning_rate - row(0 -> nlr) * ((lr_end - learning_rate) / nlr)\n",
    "\n",
    "// Initialize the games\n",
    "print(\"Initializing games...\")\n",
    "val envs = new Array[ALE](npar)\n",
    "val state = zeros(nfeats * nwindow, npar)\n",
    "var total_time=0f\n",
    "var total_steps=0\n",
    "var total_epochs = 0\n",
    "\n",
    "import java.util.Random\n",
    "val rn = new Random\n",
    "\n",
    "tic\n",
    "for (i <- 0 until npar) {\n",
    "    envs(i) = new ALE\n",
    "    envs(i).setInt(\"random_seed\", i)\n",
    "    envs(i).loadROM(game_bin)\n",
    "\n",
    "    val nmoves = rn.nextInt(init_moves - nwindow) + nwindow\n",
    "    for (j <- 0 until nmoves) {   \n",
    "        val action = VALID_ACTIONS(rn.nextInt(nactions))\n",
    "        val (obs, reward, done) = envs(i).step(action)\n",
    "        total_steps += 1;\n",
    "        if (nmoves - j <= nwindow) {\n",
    "            val k = nwindow - nmoves + j;\n",
    "            state((k*nfeats)->((k+1)*nfeats), i) = preprocess(obs)\n",
    "        }\n",
    "        if (done) {\n",
    "            envs(i).reset() \n",
    "            total_epochs += 1\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "total_time = toc     \n",
    "println(\"%d steps, %d epochs in %5.4f seconds at %5.4f msecs/step\" format(\n",
    "    total_steps, total_epochs, total_time, 1000f*total_time/total_steps))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select integer actions using the probability distribution in each column of <code>probs</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "def random_choices(probs:FMat):IMat = {\n",
    "    val result = izeros(1, probs.ncols);\n",
    "    var i = 0;\n",
    "    while (i < probs.ncols) {\n",
    "        val r = rn.nextFloat();\n",
    "        var j = 0;\n",
    "        var cumprob = probs(0, i);\n",
    "        while (r > cumprob && j+1 < probs.length) {\n",
    "            j += 1;\n",
    "            cumprob += probs(j, i);\n",
    "        }\n",
    "        result(i) = j;\n",
    "        i += 1\n",
    "    }\n",
    "    result\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implement Deep Q-Learning\n",
    "In this cell you actually implement the algorithm. We've given you comments to define all the steps. You should also add book-keeping steps to keep track of the loss, reward and number of epochs (where env.step() returns done = true). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, time 0.6, loss 0.00000234, epochs 0, reward/epoch 0.00000, cum reward/epoch 0.00000\n",
      "step 10000, time 2829.6, loss 0.07163326, epochs 433, reward/epoch -19.95843, cum reward/epoch -19.95843\n",
      "step 20000, time 5650.0, loss 0.06619549, epochs 756, reward/epoch -18.66254, cum reward/epoch -19.40476\n",
      "step 30000, time 8483.7, loss 0.06606811, epochs 1037, reward/epoch -18.49110, cum reward/epoch -19.15718\n",
      "step 40000, time 11305.7, loss 0.06578313, epochs 1312, reward/epoch -17.65818, cum reward/epoch -18.84299\n",
      "step 50000, time 14124.8, loss 0.06640674, epochs 1568, reward/epoch -17.79688, cum reward/epoch -18.67219\n",
      "step 60000, time 16948.9, loss 0.06610322, epochs 1813, reward/epoch -17.55918, cum reward/epoch -18.52179\n",
      "step 70000, time 19764.9, loss 0.06629482, epochs 2056, reward/epoch -17.13580, cum reward/epoch -18.35798\n",
      "step 80000, time 22589.6, loss 0.06544075, epochs 2284, reward/epoch -17.60526, cum reward/epoch -18.28284\n",
      "step 90000, time 25426.4, loss 0.06539343, epochs 2523, reward/epoch -16.67364, cum reward/epoch -18.13040\n",
      "step 100000, time 28263.1, loss 0.06431171, epochs 2747, reward/epoch -17.42411, cum reward/epoch -18.07281\n",
      "step 110000, time 31100.6, loss 0.06471598, epochs 2972, reward/epoch -16.76889, cum reward/epoch -17.97409\n",
      "step 120000, time 33926.6, loss 0.06438464, epochs 3193, reward/epoch -17.08597, cum reward/epoch -17.91262\n",
      "step 130000, time 36758.7, loss 0.06488869, epochs 3410, reward/epoch -16.55760, cum reward/epoch -17.82639\n",
      "step 140000, time 39593.6, loss 0.06485554, epochs 3626, reward/epoch -16.61574, cum reward/epoch -17.75427\n",
      "step 150000, time 42420.7, loss 0.06459855, epochs 3841, reward/epoch -16.76744, cum reward/epoch -17.69904\n",
      "step 160000, time 45259.1, loss 0.06357130, epochs 4047, reward/epoch -17.00000, cum reward/epoch -17.66345\n",
      "step 170000, time 48100.3, loss 0.06426635, epochs 4260, reward/epoch -16.52113, cum reward/epoch -17.60634\n",
      "step 180000, time 50932.6, loss 0.06480157, epochs 4469, reward/epoch -16.40191, cum reward/epoch -17.55001\n",
      "step 190000, time 53764.9, loss 0.06423169, epochs 4672, reward/epoch -16.78818, cum reward/epoch -17.51691\n",
      "step 200000, time 56599.6, loss 0.06426227, epochs 4887, reward/epoch -16.57674, cum reward/epoch -17.47555\n",
      "step 210000, time 59433.0, loss 0.06333619, epochs 5096, reward/epoch -16.78469, cum reward/epoch -17.44721\n",
      "step 220000, time 62265.6, loss 0.06461007, epochs 5304, reward/epoch -16.39423, cum reward/epoch -17.40592\n",
      "step 230000, time 65100.1, loss 0.06499238, epochs 5516, reward/epoch -16.56604, cum reward/epoch -17.37364\n",
      "step 240000, time 67934.5, loss 0.06378638, epochs 5717, reward/epoch -16.68657, cum reward/epoch -17.34948\n",
      "step 250000, time 70770.1, loss 0.06339571, epochs 5921, reward/epoch -16.30392, cum reward/epoch -17.31346\n",
      "step 260000, time 73604.1, loss 0.06365693, epochs 6128, reward/epoch -16.97101, cum reward/epoch -17.30189\n",
      "step 270000, time 76437.4, loss 0.06390628, epochs 6336, reward/epoch -16.33654, cum reward/epoch -17.27020\n",
      "step 280000, time 79269.2, loss 0.06344705, epochs 6533, reward/epoch -16.64975, cum reward/epoch -17.25149\n",
      "step 290000, time 82101.8, loss 0.06340136, epochs 6731, reward/epoch -16.70707, cum reward/epoch -17.23548\n",
      "step 300000, time 84933.9, loss 0.06403953, epochs 6940, reward/epoch -15.90431, cum reward/epoch -17.19539\n",
      "step 310000, time 87769.3, loss 0.06383277, epochs 7135, reward/epoch -16.61539, cum reward/epoch -17.17954\n",
      "step 320000, time 90639.4, loss 0.06265517, epochs 7335, reward/epoch -15.92500, cum reward/epoch -17.14533\n",
      "step 330000, time 93491.8, loss 0.06312279, epochs 7516, reward/epoch -16.37569, cum reward/epoch -17.12680\n",
      "step 340000, time 96353.9, loss 0.06309319, epochs 7709, reward/epoch -15.98964, cum reward/epoch -17.09833\n",
      "step 350000, time 99212.4, loss 0.06329481, epochs 7905, reward/epoch -16.35714, cum reward/epoch -17.07995\n",
      "step 360000, time 102073.8, loss 0.06323750, epochs 8095, reward/epoch -16.28947, cum reward/epoch -17.06140\n",
      "step 370000, time 104967.1, loss 0.06321532, epochs 8289, reward/epoch -15.98454, cum reward/epoch -17.03619\n",
      "step 380000, time 107844.4, loss 0.06383163, epochs 8485, reward/epoch -16.54592, cum reward/epoch -17.02487\n",
      "step 390000, time 110733.0, loss 0.06289219, epochs 8681, reward/epoch -15.79592, cum reward/epoch -16.99712\n",
      "step 400000, time 113644.0, loss 0.06241955, epochs 8865, reward/epoch -16.92391, cum reward/epoch -16.99560\n",
      "step 410000, time 116604.9, loss 0.06261902, epochs 9056, reward/epoch -15.43979, cum reward/epoch -16.96279\n",
      "step 420000, time 119478.7, loss 0.06251054, epochs 9248, reward/epoch -16.30208, cum reward/epoch -16.94907\n",
      "step 430000, time 122403.6, loss 0.06323183, epochs 9430, reward/epoch -15.96154, cum reward/epoch -16.93001\n",
      "step 440000, time 125320.0, loss 0.06264306, epochs 9614, reward/epoch -16.25543, cum reward/epoch -16.91710\n",
      "step 450000, time 128204.2, loss 0.06273638, epochs 9799, reward/epoch -15.82703, cum reward/epoch -16.89652\n",
      "step 460000, time 131140.4, loss 0.06249154, epochs 9988, reward/epoch -15.66667, cum reward/epoch -16.87325\n",
      "step 470000, time 134062.0, loss 0.06284329, epochs 10169, reward/epoch -15.82873, cum reward/epoch -16.85466\n",
      "step 480000, time 136983.0, loss 0.06407066, epochs 10356, reward/epoch -15.54011, cum reward/epoch -16.83092\n"
     ]
    }
   ],
   "source": [
    "tic\n",
    "var block_loss = 0f\n",
    "var block_reward = 0f\n",
    "var total_reward = 0f\n",
    "total_epochs = 0\n",
    "var last_epochs = 0\n",
    "val new_state = state.copy\n",
    "\n",
    "val times = zeros(1,8)\n",
    "val dtimes = zeros(1,7)\n",
    "for (istep <- 0 until nsteps) {\n",
    "//    if (render): envs[0].render()\n",
    "    times(0) = toc\n",
    "    val epsilon = epsilons(math.min(istep, neps-1));                                // get an epsilon for the eps-greedy policy\n",
    "    val lr = learning_rates(math.min(istep, nlr-1));                                // update the decayed learning rate\n",
    "    \n",
    "    update_estimator(target_estimator, q_estimator, target_window, istep);          // update the target estimator if needed    \n",
    "    times(1) = toc\n",
    "    \n",
    "    val action_probs = policy(q_estimator, state, epsilon);                         // get the next action probabilities from the policy\n",
    "    times(2) = toc\n",
    "                                                          \n",
    "    val actions = random_choices(action_probs);                                     // Choose actions using the policy\n",
    "    val (obs, rewards, dones) = ALE.stepAll2(envs, VALID_ACTIONS(actions))           // step through parallel envs\n",
    "    times(3) = toc\n",
    "    \n",
    "    for (i <- 0 until npar) {                                                     \n",
    "        val img = preprocess(obs(i));                                               // process the observation\n",
    "        new_state(?,i) = state(nfeats->state.nrows,i) on img;                       // add it to buffer of last nwindow imgs        \n",
    "    }    \n",
    "    total_epochs += sum(dones).v.toInt\n",
    "    block_reward += sum(rewards).v  \n",
    "    times(4) = toc\n",
    "    \n",
    "    val q_values_next = target_estimator.predict(new_state);                        // predict the Q values\n",
    "    times(5) = toc\n",
    "    \n",
    "    dones <-- (dones + (rewards != 0f) > 0f);\n",
    "    val targets = rewards+discount_factor*(1f-dones) *@ maxi(q_values_next);        // compute target values   \n",
    "    block_loss += q_estimator.gradient(state, actions, targets);                    // compute q-estimator gradient and return the loss\n",
    "    times(6) = toc\n",
    "    \n",
    "    if (istep % rmsevery == 0) {\n",
    "        q_estimator.msprop(lr, gsq_decay);                       // apply the gradient update\n",
    "//        print(\"ds1=%f, ss1=%f, ds2=%f, ss2=%f\\n\" format(res(0,0), res(1,0), res(0,1), res(1,1)));\n",
    "    }\n",
    "    times(7) = toc\n",
    "    \n",
    "    dtimes ~ dtimes + (times(0,1->8) - times(0,0->7))\n",
    "    val t = toc;\n",
    "    if (istep % printsteps == 0) {\n",
    "        total_reward += block_reward;\n",
    "        println(\"step %d, time %2.1f, loss %9.8f, epochs %d, reward/epoch %6.5f, cum reward/epoch %6.5f\" format(\n",
    "                istep, t, block_loss/printsteps, total_epochs, block_reward/math.max(1,total_epochs-last_epochs), total_reward/math.max(1,total_epochs)))\n",
    "        last_epochs = total_epochs;\n",
    "        block_reward = 0f;\n",
    "        block_loss = 0f;\n",
    "    }\n",
    "    state <-- new_state;\n",
    "}\n",
    "dtimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And animate the model's performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val layer=\"W1\"\n",
    "q_estimator.model(layer).dims on q_estimator.grad(layer).dims \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "/*state0 = state[:,0]\n",
    "for i in np.arange(200):\n",
    "    envs[0].render()\n",
    "    preds = test_estimator.predict(state0)\n",
    "    iaction = np.argmax(preds)\n",
    "    obs, _, done0, _ = envs[0].step(VALID_ACTIONS[iaction])\n",
    "    state0 = np.concatenate((state0[nfeats:], preprocess(obs)))\n",
    "    if (done0): envs[0].reset() */\n",
    "//hist(ln(abs(q_estimator.gradsq(\"W1\")(?))),100)\n",
    "//    hist(ln(abs(q_estimator.gradsq(\"W1\")(?))),100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there we have it. Simple 1-step Q-Learning can solve easy problems very fast. Note that environments that produce images will be much slower to train on than environments (like CartPole) which return an observation of the state of the system. But this model can still train on those image-based games - like Atari games. It will take hours-days however. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//q_estimator.gradient(state,actions,targets)\n",
    "q_estimator.model(\"W2\").dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//saveFMat(\"BestW1.fmat.lz4\",q_estimator.model(\"W1\"))\n",
    "//saveFMat(\"BestW2.fmat.lz4\",q_estimator.model(\"W2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val test_steps = 100001;\n",
    "val testprintsteps = 10000;\n",
    "\n",
    "val test_estimator = new Estimator(nfeats*nwindow, nhidden, nactions);\n",
    "test_estimator.model(\"W1\") = loadFMat(\"BestW1.fmat.lz4\");\n",
    "test_estimator.model(\"W2\") = loadFMat(\"BestW2.fmat.lz4\");\n",
    "\n",
    "block_reward = 0f;\n",
    "total_reward = 0f;\n",
    "total_epochs = 0;\n",
    "last_epochs = 0;\n",
    "\n",
    "tic;\n",
    "for (istep <- 0 until test_steps) {\n",
    "    \n",
    "    val action_probs = policy(test_estimator, state, 0);                    // get the next action probabilities from the policy                                                         \n",
    "    val actions = random_choices(action_probs);                             // Choose actions using the policy\n",
    "    val (obs, rewards, dones) = ALE.stepAll2(envs, VALID_ACTIONS(actions))  // step through parallel envs   \n",
    "    for (i <- 0 until npar) {                                                     \n",
    "        val img = preprocess(obs(i));                                       // process the observation\n",
    "        state(?,i) = state(nfeats->state.nrows,i) on img;                   // add it to buffer of last nwindow imgs        \n",
    "    }    \n",
    "    total_epochs += sum(dones).v.toInt\n",
    "    block_reward += sum(rewards).v  \n",
    "    \n",
    "    val t = toc;\n",
    "    if (istep % testprintsteps == 0) {\n",
    "        total_reward += block_reward;\n",
    "        println(\"step %d, time %2.1f, epochs %d, reward/epoch %6.5f, cum reward/epoch %6.5f\" format(\n",
    "                istep, t, total_epochs, block_reward/math.max(1,total_epochs-last_epochs), total_reward/math.max(1,total_epochs)))\n",
    "        last_epochs = total_epochs;\n",
    "        block_reward = 0f;\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "name": "scala",
   "version": "2.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
