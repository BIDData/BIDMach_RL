:silent 
import BIDMach.rl.ALE;
import jcuda.jcudnn.cudnnTensorFormat._
import jcuda.jcudnn.cudnnConvolutionMode._
import java.util.Random

val nsteps = 2000001                     // Number of steps to run (game actions per environment)
val npar = 16                            // Number of parallel environments
val target_window = 50                   // Interval to update target estimator from q-estimator
val discount_factor = 0.99f              // Reward discount factor
val printsteps = 10000                   // Number of steps between printouts
val render = false                       // Whether to render an environment while training

val epsilon_start = 0.5f                 // Parameters for epsilon-greedy policy: initial epsilon
val epsilon_end = 0.1f                   // Final epsilon
val neps = (0.9*nsteps).toInt            // Number of steps to decay epsilon

val learning_rate = 1e-4f
val learning_rate = 1e-6f                // Initial learning rate
//val learning_rate = 1e-7f                // Initial learning rate
val lr_end = learning_rate               // Final learning rate
val nlr = neps                           // Steps to decay learning rate
val gsq_decay = 0.99f                    // Decay factor for MSProp
val vel_decay = 0.0f                     // Momentum decay
val gclip = 1f                           // gradient clipping
val rmsevery = 5                         // apply gradient after skipping this many steps
val baseline_decay = 0.9999f

val nhidden = 16                         // Number of hidden layers for estimators
val nhidden2 = 32
val nhidden3 = 256

val init_moves = 10000                   // Upper bound on random number of moves to take initially
val nwindow = 4                          // Sensing window = last n images in a state
val history_size = 4e9f                  // History size in Bytes

// The epsilon and learning rate decay schedules
// val epsilons = np.linspace(epsilon_start, epsilon_end, neps)

val epsilons = epsilon_start / (1f + row(0->neps)/(neps*epsilon_end/epsilon_start))
val learning_rates = learning_rate - row(0 -> nlr) * ((lr_end - learning_rate) / nlr)


// Model type and action definitions

val game_bin="/code/ALE/roms/Pong.bin"                 
val VALID_ACTIONS = irow(0, 3, 4)
val nactions= VALID_ACTIONS.length
val height = 80
val width = 80

val max_history = (history_size/(height*width*npar*4)).toInt

def preprocess(img:FMat, out:FMat):FMat = {
//  Preprocess Pong game frames into vectors.
//  Input:
//    - (3,160,210) FMat frame representing Pong game screen.
//  Returns:
//    - Downsampled (DxD) matrix of brightness values
    var i = 0;
    val res = if (out.asInstanceOf[AnyRef] == null) zeros(width\height\1\1) else out;
    while (i < height) {
        var j = 0;
        while (j < width) {
	    val indx = 3*(j*2 + 160 * (i*2 + 35))
            val y = (img(indx) + img(indx+1) + img(indx+2)).toFloat
            res.data(j + width*i) = y / 765f;
            j += 1;
        }
        i += 1;
    }
    res
}


def preprocess2(I:Array[Byte], out:FMat):FMat = {
//  Preprocess Pong game frames into vectors.
//  Input:
//    - (3,160,210) uint8 frame representing Pong game screen.
//  Returns:
//    - Downsampled (DxD) matrix of 0s and 1s
    var i = 0;
    val res = if (out.asInstanceOf[AnyRef] == null) zeros(width\height\1\1) else out;
    while (i < height) {
        var j = 0;
        while (j < width) {
            val x = I(j*2 + 160 * (i*2 + 35));
            res.data(j + width*i) = {if (x == 34) 0f else {if (x != 0) 1f else 0f}};
            j += 1;
        }
        i += 1;
    }
    res
}


val convt = CUDNN_CROSS_CORRELATION;
val tformat = Net.TensorNCHW;

class Myopts extends Net.Options with ADAGrad.Opts;

class Estimator(nhidden:Int, nhidden2:Int, nhidden3:Int, nactions:Int) {
    
    val opts = new Myopts;  
    opts.vexp = 1f;
    opts.texp = 0f;
    opts.waitsteps = -1;
    opts.vel_decay = vel_decay;
    opts.gsq_decay = gsq_decay;
    opts.tensorFormat = tformat;
    
    import BIDMach.networks.layers.Node._;

    val in =    input;
    val acts =  input;
    
    val conv1 = conv(in)(w=7,h=7,nch=nhidden,stride=4,pad=3,initv=1f,convType=convt);
    val relu1 = relu(conv1);

    val conv2 = conv(relu1)(w=3,h=3,nch=nhidden2,stride=2,pad=1,convType=convt);
    val relu2 = relu(conv2);

    val fc3 =   linear(relu2)(outdim=nhidden3,initv=2e-2f);
    val relu3 = relu(fc3);
    
    val fc4 =   linear(relu3)(outdim=nactions,initv=5e-2f); 
    val npredict = 9;                                       // Number of layers for action prediction. 
    
    val sel =   fc4(acts);
    
    val out =   glm(sel)(izeros(1,1))

    val nodes = (in     \ acts   on
                 conv1  \ relu1  on
                 conv2  \ relu2  on
                 fc3    \ relu3  on
                 fc4    \ null   on
                 sel    \ out    ).t

    opts.nodemat = nodes;
    
    val net = new Net(opts);
    val adagrad = new ADAGrad(opts);
    
    def formatStates(s:FMat):FMat = {
        if (tformat == Net.TensorNCHW) {
            s.reshapeView(nwindow\height\width\npar);
        } else {
            s.reshapeView(height\width\nwindow\npar).transpose(2\0\1\3);
        }
    }
    
/** Perform the initialization that is normally done by the Learner */

    var initialized = false;
    
    def checkinit(states:FMat, actions:IMat, rewards:FMat) = {
	if (net.mats.asInstanceOf[AnyRef] == null) {
	    net.mats = new Array[Mat](3);
	    net.gmats = new Array[Mat](3);
	}
	net.mats(0) = states;
	if (net.mats(1).asInstanceOf[AnyRef] == null) {
	    net.mats(1) = izeros(1, states.ncols);              // Dummy action vector
	    net.mats(2) = zeros(1, states.ncols);               // Dummy reward vector
	}
	if (actions.asInstanceOf[AnyRef] != null) {
	    net.mats(1) <-- actions;
	}
	if (rewards.asInstanceOf[AnyRef] != null) {
	    net.mats(2) <-- rewards;
	}
        if (!initialized) {
	    net.useGPU = (opts.useGPU && Mat.hasCUDA > 0);
	    net.init();
            adagrad.init(net);
            initialized = true;
        }
	net.copyMats(net.mats, net.gmats);
	net.assignInputs(net.gmats, 0, 0);
	net.assignTargets(net.gmats, 0, 0);
    }
    
/**  Run the model forward given a state as input up to the action prediction layer. 
     Action selection/scoring layers are not updated.
     returns action predictions */
    
    def predict(states:FMat):FMat = {
	val fstates = formatStates(states);
        checkinit(fstates, null, null);
        for (i <- 0 until npredict) net.layers(i).forward;
        FMat(net.layers(npredict-1).output);
    }

/** Run the model all the way forward to the squared loss output layer, 
    and then backward to compute gradients.
    An action vector and reward vector must be given. */  
    
def gradient(states:FMat, actions:IMat, rewards:FMat, ndout:Int=npar):Float = {
	val fstates = formatStates(states);
        checkinit(fstates, actions, rewards);
	net.forward;
	net.setderiv(ndout);
        net.backward(0, 0);
        val dout = net.layers(net.layers.length-1).score;
        sqrt(-dout/dout.length).fv                             // return the RMS residual error. 
    }
        
/** MSprop, i.e. RMSprop without the square root, or natural gradient */
        
    def msprop(learning_rate:Float) = {                
        opts.lrate = learning_rate;
        adagrad.update(0,0,0);
	net.cleargrad;
	
    }

    val aselector = irow(0->npar)*nactions;
    val A = zeros(nactions, npar);

    def policy(state:FMat, epsilon:Float):FMat = {
//    """ Take an estimator and state and predict the best action.
//    For each input state, return a vector of action probabilities according to an epsilon-greedy policy"""
	A.set(epsilon / nactions);
	val q_values = predict(state);
	val (_,best_action) = maxi2(q_values)
	A(best_action + aselector) = A(best_action + aselector) + (1f - epsilon)
	A
    }

    def update_estimator(to_estimator:Estimator, window:Int, istep:Int) = {
//    """ every <window> steps, Copy model state from from_estimator into to_estimator"""
	if (istep % window == 0) {
	    for (k  <- 0 until net.modelmats.length) {
		to_estimator.net.modelmats(k) <-- net.modelmats(k);
	    }
	}
    }

};


// Initialize the games
print("Initializing games...");
val envs = new Array[ALE](npar);
val state = zeros(width\height\nwindow\npar);
val img0 = zeros(width\height\1\1);
var total_time=0f;
var total_steps=0;
var total_epochs = 0;
var tbaseline = 0f;
var block_count = 0;
var block_reward = 0f

val rn = new Random

tic
for (i <- 0 until npar) {
    envs(i) = new ALE
    envs(i).setInt("random_seed", i)
    envs(i).loadROM(game_bin)

    val nmoves = rn.nextInt(init_moves - nwindow) + nwindow
    for (j <- 0 until nmoves) {   
        val action = VALID_ACTIONS(rn.nextInt(nactions))
        val (obs, reward, done) = envs(i).step2(action)
        total_steps += 1;
        if (nmoves - j <= nwindow) {
            val k = nwindow - nmoves + j;
            state(?,?,k,i) = preprocess2(obs, img0)
        }
	if (done || reward != 0) {
	    block_reward += reward;
	    block_count += 1;
	}
        if (done) {
            envs(i).reset() 
            total_epochs += 1
        }
    }
}

total_time = toc     
tbaseline = block_reward/block_count;
println("%d steps, %d epochs in %5.4f seconds at %5.4f msecs/step, R/B = %5.4f" format(
	 total_steps, total_epochs, total_time, 1000f*total_time/total_steps, tbaseline))


// Create estimators
val q_estimator = new Estimator(nhidden, nhidden2, nhidden3, nactions);

// Initialize them by making predictions
q_estimator.predict(state);

def random_choices(probs:FMat):IMat = {
    val result = izeros(1, probs.ncols);
    var i = 0;
    while (i < probs.ncols) {
        val r = rn.nextFloat();
        var j = 0;
        var cumprob = probs(0, i);
        while (r > cumprob && j+1 < probs.nrows) {
            j += 1;
            cumprob += probs(j, i);
        }
        result(i) = j;
        i += 1
    }
    result
}

tic
var block_loss = 0f
block_count = 0;
block_reward = 0f
var total_reward = 0f
total_epochs = 0
var last_epochs = 0
val new_state = state.copy
val history = zeros(width\height\max_history\npar);
val action_history = izeros(max_history,npar);
val history_ptr = izeros(1,npar);
Mat.useGPUcache = true;

val times = zeros(1,6);
val dtimes = zeros(1,5);
val rand_actions = int(rand(1, npar) * nactions);
val (obs0, rewards0, dones0) = ALE.stepAll2(envs, VALID_ACTIONS(rand_actions));           // step through parallel envs
var actions:IMat = null;
var action_probs:FMat = null;
val running_rewards = zeros(1, npar);


def thread_gradient(ithread:Int, reward:Float):(Float,Int) = {
    var i = 0;
    var iblocks = 0;
    var block_loss = 0f;
    val targets = zeros(1,npar);
    targets.set(reward);
    while (i < history_ptr(ithread) - npar - nwindow - 1) {
	var j = 0;
	while (j < npar) {
	    state(?,?,?,j) = history(?,?,(i+j)->(i+j+nwindow), ithread);
	    actions(0,j) = action_history(i+j+nwindow+1, ithread);
	    j += 1;
	}
	block_loss += q_estimator.gradient(state, actions, targets);                // compute q-estimator gradient and return the loss
	iblocks += 1;
	i += npar;
    }
    (block_loss, iblocks);
}

def showx(state:FMat) = {
    val st0 = state(?,?,1->4,?).transpose(2\0\3\1);
    st0(?,79,?,?) = 1f;
    val st = st0.reshapeView(3,80*npar,80)*255f;
    show(st)
    st;
}

def showy(t:Int, i:Int) = {
    if (history_ptr(i) > t + 24) {
    val dd = history(?,?,t->(t+8*3),i).reshapeView(80,80,3,8);
    dd(79,?,?,?) = 1f;
    val st = dd.transpose(2\0\3\1).reshapeView(3,80*8,80)*255f;
    show(st);
    action_history(t->(t+8*3),i).t.reshapeView(3,8);
    } else {irow(0)}
}


for (istep <- 0 until nsteps) {
//    if (render): envs[0].render()
    times(0) = toc
    val epsilon = epsilons(math.min(istep, neps-1));                                // get an epsilon for the eps-greedy policy
    val lr = learning_rates(math.min(istep, nlr-1));                                // update the decayed learning rate
    
    action_probs = q_estimator.policy(state, epsilon);                              // get the next action probabilities from the policy
    times(1) = toc
                                                 
    actions = random_choices(action_probs);                                         // Choose actions using the policy
    val (obs, rewards, dones) = ALE.stepAll2(envs, VALID_ACTIONS(actions), obs0, rewards0, dones0)           // step through parallel envs
    times(2) = toc

    total_epochs += sum(dones).v.toInt;
    block_reward += sum(rewards).v;

    dones <-- (dones + (rewards != 0f) > 0f);                                       // For Pong only. Finish epoch at any reward

    for (i <- 0 until npar) {                                                     
        val img = preprocess2(obs(i), img0);                                        // process the observation
        new_state(?,?,0->(nwindow-1),i) = state(?,?,1->nwindow,i);                  // shift the image stack and add a new image
	new_state(?,?,nwindow-1,i) = img;      
	history(?,?,history_ptr(i),i) = img;                                        // Save in history buffer
	action_history(history_ptr(i),i) = actions(i);
	running_rewards(i) += rewards(i);
	history_ptr(i) += 1;
    }

    times(3) = toc;
    times(4) = toc;

    for (i <- 0 until npar) {
	if (dones(i) > 0) {
	    tbaseline = baseline_decay * tbaseline + (1-baseline_decay) * running_rewards(i);

	    val (block_loss0, block_cnt0) = thread_gradient(i, running_rewards(i) - tbaseline);
	    times(4) = toc;
	    block_loss += block_loss0;
	    block_count += block_cnt0;
	    q_estimator.msprop(lr);                       // apply the gradient update
	    running_rewards(i) = 0;
	    history_ptr(i) = 0;
	} else if (history_ptr(i) > max_history) {
	    running_rewards(i) = 0;
	    history_ptr(i) = 0;
	    envs(i).reset();
	}
    }        

    times(5) = toc
    
    dtimes ~ dtimes + (times(0,1->6) - times(0,0->5))
    val t = toc;
    if (istep % printsteps == 0) {
        total_reward += block_reward;
        println("step %d, time %2.1f, loss %7.6f, B %d, E %d, R/E %6.5f, CR/E %6.5f" format(
		istep, t, block_loss/math.max(1,block_count),block_count, total_epochs, 
                block_reward/math.max(1,total_epochs-last_epochs), total_reward/math.max(1,total_epochs)))
        last_epochs = total_epochs;
        block_reward = 0f;
        block_loss = 0f;
	block_count = 0;
    }
    state <-- new_state;
    //    Mat.debugMem = true;
    //    Mat.debugMemThreshold = 0;
}
Mat.useGPUcache=false
:silent
dtimes
