:silent
import BIDMach.rl.ALE;
import jcuda.jcudnn.cudnnTensorFormat._
import jcuda.jcudnn.cudnnConvolutionMode._
import java.util.Random

val nsteps = 400000;                     // Number of steps to run (game actions per environment)
val npar = 16;                            // Number of parallel environments
val ndqn = 5;                             // Number of DQN steps per update
val target_window = 50;                   // Interval to update target estimator from q-estimator
val discount_factor = 0.99f;              // Reward discount factor
val printsteps = 10000;                   // Number of steps between printouts
val render = false;                       // Whether to render an environment while training
val nwindow = 4;                          // Sensing window = last n images in a state
:silent

val epsilon_schedule = (0f \ 0.2f on
			0.8f \ 0.02f on
			0.8f \ 3e-10f on
			1f \ 3e-10f); 

val lr_schedule = (0f \ 3e-6f on
		   0.8f \ 3e-6f on
		   0.8f \ 3e-10f on
		   1f \ 3e-10f); 

val lambda_schedule = (0f \ 100f on 
                       1f \ 100f);

val temp_schedule = (0f \ 0.0001f on 
                     1f \ 0.0001f);

val avarv = 1e-10f;                      // Variance regularization weight
val entropyv = 1e-7f;                     // Entropy regularization weight

val neps = nsteps.toInt;            // Number of steps to decay epsilon
:silent
val nlr = neps                           // Steps to decay learning rate
val avar_decay = 0.999f;
val credit_decay = 0.999f;
val gsq_decay = 0.99f                    // Decay factor for MSProp
val vel_decay = 0.9f                     // Momentum decay
val baseline_decay = 0.9999f;
val gclip = 1f;                           // gradient clipping
var avar_avg = 0.0001f;
var credit_avg = 0.0001f;

val nhidden = 16;                         // Number of hidden layers for estimators
val nhidden2 = 32;
val nhidden3 = 256;

val init_moves = 4000;                   // Upper bound on random number of moves to take initially

// The epsilon and learning rate decay schedules
// val epsilons = np.linspace(epsilon_start, epsilon_end, neps)


// Model type and action definitions

val game_bin="/code/ALE/roms/Pong.bin";                 
val VALID_ACTIONS = irow(0, 3, 4);
val nactions= VALID_ACTIONS.length;
val height = 80;
val width = 80;
val frameskip = (4,4);
val repeat_action_probability = 0f;
val score_range = row(-1f,1f);

def linterp(schedule:FMat, npoints:Int) = {
    val out = zeros(1, npoints);
    var i = 0;
    var isched = 0;
    while (i < npoints) {
	val vv = i*1.0f/npoints;
	while (isched+1 < schedule.nrows && vv > schedule(isched+1, 0)) {
	    isched += 1;
	}
	val frac = (vv - schedule(isched, 0))/(schedule(isched+1, 0)-schedule(isched, 0));
	out.data(i) = math.exp(frac * math.log(schedule(isched+1, 1)) + (1-frac) * math.log(schedule(isched, 1))).toFloat;
	i += 1;
    }
    out;
};

val learning_rates = linterp(lr_schedule, nlr);
val epsilons = linterp(epsilon_schedule, neps);
val lambdas = linterp(lambda_schedule, neps);
val temperatures = linterp(temp_schedule, neps);

val save_length = 1000;
val saved_games = zeros(width\height\save_length\npar);
val saved_actions = izeros(save_length,npar);
val saved_preds = zeros(nactions\save_length\npar);
var igame = 0;


def preprocess2(I:Array[Byte], out:FMat):FMat = {
//  Preprocess Pong game frames into vectors.
//  Input:
//    - (3,160,210) uint8 frame representing Pong game screen.
//  Returns:
//    - Downsampled (DxD) matrix of 0s and 1s
    var i = 0;
    val res = if (out.asInstanceOf[AnyRef] == null) zeros(width\height\1\1) else out;
    res.clear
    while (i < height*2) {
        var j = 0;
	val i2 = i >> 1;
        while (j < width*2) {
	    val j2 = j >> 1;	
            val x = I(j + 160 * (i + 35));
	    val y = {if (x == 34) 0f else {if (x != 0) 1f else 0f}};
            res.data(j2 + width*i2) += y * 0.25f;
            j += 1;
        }
        i += 1;
    }
    res
}

val convt = CUDNN_CROSS_CORRELATION;
val tformat = Net.TensorNCHW;

class Myopts extends Net.Options with ADAGrad.Opts;

class Estimator(nhidden:Int, nhidden2:Int, nhidden3:Int, nactions:Int) {
    
    val opts = new Myopts;  
    opts.vexp = 1f;
    opts.texp = 0f;
    opts.waitsteps = -1;
    opts.vel_decay = vel_decay;
    opts.gsq_decay = gsq_decay;
    opts.tensorFormat = tformat;


    var ipred = 0;
    var iprob = 0;
    var iloss = 0;
    var iavar = 0;
    var icredit = 0;
    var ientropy = 0;
    
    {
	import BIDMach.networks.layers.Node._;
	// Input layers
	val in =      input;
	val acts =    input;
	val target =  input;
	val ptemp  =  constant(1/temperatures(0));

	// Convolution layers
	val conv1 =   conv(in)(w=7,h=7,nch=nhidden,stride=4,pad=3,initv=1f,convType=convt);
	val relu1 =   relu(conv1);
	val conv2 =   conv(relu1)(w=3,h=3,nch=nhidden2,stride=2,pad=0,convType=convt);
	val relu2 =   relu(conv2);

	// FC/reward prediction layers
	val fc3 =     linear(relu2)(outdim=nhidden3,initv=2e-2f);
	val relu3 =   relu(fc3);
	val rpred =   linear(relu3)(outdim=nactions,initv=5e-2f); 
	ipred =       10;
	val rpredt =  rpred *@ ptemp;

	// Probability/constant layers
	val aprob =   softmax(rpredt);
	iprob =       12;                                        // Index of action policy layer
	val minus1 =  constant(-1f);
	val athird =  constant(1f/3f);
	val rsum =    sum(rpred);                           

	// Advantage/variance
	val rmean =   rsum *@ athird;
	val advtg =   rpred - rmean;                             // Advantage
	val avar0 =   advtg dot advtg;
	val avar  =   avar0 *@ athird;
	iavar =       19;                                        // Index of variance layer

	// Normalized credit layers
	val iwavg =   constant(0.001f);                     // Assumed to be at index iavar+1      
	val lambdav = constant(lambdas(0));                    // Assumed to be at index iavar+2
	val xavar =   avar *@ iwavg;
	val yavar =   xavar + lambdav

	val savar =   tanh(yavar);
	icredit =     24;                                        // Index of tanh credit layer
	val fcredit = forward(savar);
	// Entropy layers
	val eps =     constant(1e-6f);
	val aprobeps =aprob + eps;

	val lnaprob = ln(aprobeps);
	val negent =  lnaprob dot aprob;
	val entropy = negent *@ minus1;
	ientropy =    30;                                        // Index of entropy layer
	val aweight = constant(-avarv);

	val avarw =   savar *@ aweight;
	val eweight = constant(entropyv);
	val entropyw= entropy *@ eweight;
	// Action loss layers
	val rpreda =  rpred(acts);

	val diff =    target - rpreda;
	val loss =    diff *@ diff; 
        iloss =       37;                                        // Index of base loss layer.
	// Total weighted loss layers
	val nloss =   loss *@ minus1;
	val wloss  =  nloss *@ fcredit;

	val out0 =    avarw + wloss;                             // No weighting
	val out =     out0 + entropyw;

	opts.nodemat = (in       \ acts     \ target   \ ptemp    on
		        conv1    \ relu1    \ conv2    \ relu2    on
		        fc3      \ relu3    \ rpred    \ rpredt   on
			aprob    \ minus1   \ athird   \ rsum     on
			rmean    \ advtg    \ avar0    \ avar     on
			iwavg    \ lambdav  \ xavar    \ yavar    on
			savar    \ fcredit  \ eps      \ aprobeps on
			lnaprob  \ negent   \ entropy  \ aweight  on 
			avarw    \ eweight  \ entropyw \ rpreda   on
                        diff     \ loss     \ nloss    \ wloss    on
                        out0     \ out      \ null     \ null     ).t
     }

    val net = new Net(opts);
    val adagrad = new ADAGrad(opts);
    
    def formatStates(s:FMat):FMat = {
        if (tformat == Net.TensorNCHW) {
            s.reshapeView(nwindow\height\width\npar);
        } else {
            s.reshapeView(height\width\nwindow\npar).transpose(2\0\1\3);
        }
    }
    
/** Perform the initialization that is normally done by the Learner */

    var initialized = false;
    
    def checkinit(states:FMat, actions:IMat, rewards:FMat) = {
	if (net.mats.asInstanceOf[AnyRef] == null) {
	    net.mats = new Array[Mat](3);
	    net.gmats = new Array[Mat](3);
	}
	net.mats(0) = states;
	if (net.mats(1).asInstanceOf[AnyRef] == null) {
	    net.mats(1) = izeros(1, states.ncols);              // Dummy action vector
	    net.mats(2) = zeros(1, states.ncols);               // Dummy reward vector
	}
	if (actions.asInstanceOf[AnyRef] != null) {
	    net.mats(1) <-- actions;
	}
	if (rewards.asInstanceOf[AnyRef] != null) {
	    net.mats(2) <-- rewards;
	}
        if (!initialized) {
	    net.useGPU = (opts.useGPU && Mat.hasCUDA > 0);
	    net.init();
            adagrad.init(net);
            initialized = true;
        }
	net.copyMats(net.mats, net.gmats);
	net.assignInputs(net.gmats, 0, 0);
	net.assignTargets(net.gmats, 0, 0);
    }
    
/**  Run the model forward given a state as input up to the action prediction layer. 
     Action selection/scoring layers are not updated.
     returns action predictions */
     def predict(states:FMat):(FMat,FMat,FMat,FMat,FMat) = {
	val fstates = formatStates(states);
        checkinit(fstates, null, null);
        for (i <- 0 to ientropy) net.layers(i).forward;
	(FMat(net.layers(ipred).output), 
	 FMat(net.layers(iprob).output), 
	 FMat(net.layers(iavar).output), 
	 FMat(net.layers(icredit).output),
	 FMat(net.layers(ientropy).output));
    }

/** Run the model all the way forward to the squared loss output layer, 
    and then backward to compute gradients.
    An action vector and reward vector must be given. */  

    def gradient(states:FMat, actions:IMat, rewards:FMat, ndout:Int=npar):(Float,Float,Float,Float) = {
	val fstates = formatStates(states);
        checkinit(fstates, actions, rewards);
	net.forward;
	net.setderiv(ndout);
        net.backward(0, 0);
        val lout = cpu(net.layers(iloss).output)(0,0->ndout);      // base loss
        val vout = cpu(net.layers(iavar).output)(0,0->ndout);      // raw weight
        val wout = cpu(net.layers(icredit).output)(0,0->ndout);    // credit
        val eout = cpu(net.layers(ientropy).output)(0,0->ndout);   // entropy
	val lv = sum(lout).fv                               // return the regression loss
	val vv = sum(vout).fv                               // return the mean weight
	val wv = sum(wout).fv                               // return the mean normalized weight
	val ev = sum(eout).fv                               // return the avg entropy
	(lv, vv, wv, ev);        
    }
    
  /*    def gradient(states:FMat, actions:IMat, rewards:FMat):Float = {
	val fstates = formatStates(states);
        checkinit(fstates, actions, rewards);
	net.forward;
	net.setderiv();
        net.backward(0, 0);
        val dout = net.layers(net.layers.length-1).output;
      (-sum(dout)/dout.length).fv                             // return the avg residual error. 
      }*/
        
/** MSprop, i.e. RMSprop without the square root, or natural gradient */
        
    def msprop(learning_rate:Float) = {                
        opts.lrate = learning_rate;
        adagrad.update(0,0,0);
	net.cleargrad;
	
    }

//    Take an estimator and state and predict the actions
    def setconsts(temperature:Float, avar_avg:Float, lambdav:Float) = {
	net.layers(3).asInstanceOf[BIDMach.networks.layers.ConstantLayer].opts.value =  1f/temperature;
	net.layers(iavar+1).asInstanceOf[BIDMach.networks.layers.ConstantLayer].opts.value =  1f/avar_avg;
	net.layers(iavar+2).asInstanceOf[BIDMach.networks.layers.ConstantLayer].opts.value =  lambdav;
    }

    def update_estimator(to_estimator:Estimator, window:Int, istep:Int) = {
//    every <window> steps, Copy model state from from_estimator into to_estimator
	if (istep % window == 0) {
	    for (k  <- 0 until net.modelmats.length) {
		to_estimator.net.modelmats(k) <-- net.modelmats(k);
	    }
	}
    }

};


// Initialize the games
print("Initializing games");
val envs = new Array[ALE](npar);
val state = zeros(width\height\nwindow\npar);
val img0 = zeros(width\height\1\1);
var obs0:Array[Byte] = null;
var total_time=0f;
var total_steps=0;
var total_epochs = 0;
var block_reward = 0f;
var block_count = 0;
val reward_plot = zeros(1, nsteps/printsteps);
val rn = new Random;

tic;
for (i <- 0 until npar) {
    envs(i) = new ALE;
    envs(i).setInt("random_seed", i);
    envs(i).setFloat("repeat_action_probability",repeat_action_probability);
    envs(i).frameskip = frameskip;
    envs(i).loadROM(game_bin);

    val nmoves = rn.nextInt(init_moves - nwindow) + nwindow
    for (j <- 0 until nmoves) {   
        val action = VALID_ACTIONS(rn.nextInt(nactions));
        val (obs, reward, done) = envs(i).step2(action);
	obs0 = obs;
        total_steps += 1;
        if (nmoves - j <= nwindow) {
            val k = nwindow - nmoves + j;
            state(?,?,k,i) = preprocess2(obs, img0)
        }
	if (done || reward != 0) {
	    block_reward += reward;
	    block_count += 1;
	}
        if (done) {
            envs(i).reset() 
            total_epochs += 1;
        }
    }
    print(".");
}

var rbaseline = block_reward/block_count.toFloat;
var rbaseline0 = rbaseline;
total_time = toc;     
println("\n%d steps, %d epochs in %5.4f seconds at %5.4f msecs/step" format(
    total_steps, total_epochs, total_time, 1000f*total_time/total_steps))


// Create estimators
val q_estimator = new Estimator(nhidden, nhidden2, nhidden3, nactions);
val t_estimator = new Estimator(nhidden, nhidden2, nhidden3, nactions);

// Initialize them by making predictions
q_estimator.predict(state);
t_estimator.predict(state);

def random_choices(probs:FMat):IMat = {
    val result = izeros(1, probs.ncols);
    var i = 0;
    while (i < probs.ncols) {
        val r = rn.nextFloat();
        var j = 0;
        var cumprob = probs(0, i);
        while (r > cumprob && j+1 < probs.nrows) {
            j += 1;
            cumprob += probs(j, i);
        }
        result(i) = j;
        i += 1
    }
    result
}

tic;
var block_loss = 0f;
var block_vloss = 0f;
var block_wloss = 0f;
var block_entropy = 0f;
var block_reward = 0f;
var total_reward = 0f;
total_epochs = 0;
var last_epochs = 0;
val new_state = state.copy;
var dobaseline = false;
val baselinethresh  = 0.1f;
Mat.useGPUcache = true;


val times = zeros(1,8);
val dtimes = zeros(1,7);
val ractions = int(rand(1, npar) * nactions);
val (obs0, rewards0, dones0) = ALE.stepAll2(envs, VALID_ACTIONS(ractions));           // step through parallel envs
var actions = izeros(1,npar);
var action_probs:FMat = null;
val rand_actions = ones(nactions, npar) * (1f/nactions);
val targwin = target_window / ndqn * ndqn; 
val printsteps0 = printsteps / ndqn * ndqn; 
val state_memory = zeros(width\height\nwindow\(npar*ndqn));
val action_memory = izeros(ndqn\npar);
val reward_memory = zeros(ndqn\npar);
val done_memory = zeros(ndqn\npar);


for (istep <- ndqn to nsteps by ndqn) {
//    if (render): envs[0].render()
    val lr = learning_rates(math.min(istep, nlr-1));                                // update the decayed learning rate
    val epsilon = epsilons(math.min(istep, neps-1));                                // get an epsilon for the eps-greedy policy
    val lambda = lambdas(math.min(istep, neps-1));                               
    val temp = temperatures(math.min(istep, neps-1));                                // get an epsilon for the eps-greedy policy
    q_estimator.setconsts(temp, avar_avg, lambda);
    t_estimator.setconsts(temp, avar_avg, lambda);

    q_estimator.update_estimator(t_estimator, targwin, istep);          // update the target estimator if needed    
    times(1) = toc;
    
    for (i <- 0 until ndqn) {
	times(0) = toc;
	val (preds, aprobs, avar0, credit0, entropy0) = q_estimator.predict(state); // get the next action probabilities etc from the policy
	avar_avg = avar_decay * avar_avg + (1f - avar_decay) * mean(avar0).v;
	credit_avg = credit_decay * credit_avg + (1f - credit_decay) * mean(credit0).v;
	times(1) = toc;
                                                 
	val dobranch = (rand(1, npar) < epsilon);                                  // Indicator to branch now
	val newprobs = dobranch *@ rand_actions + (1-dobranch) *@ aprobs;               // Updated probabilities including eps-greedy
	actions <-- multirnd(newprobs);                                              // Choose actions using the policy 
	val (obs, rewards, dones) = ALE.stepAll2(envs, VALID_ACTIONS(actions), obs0, rewards0, dones0);           // step through parallel envs
	times(2) = toc;

	for (j <- 0 until npar) {                                                     
	    val img = preprocess2(obs(j), img0);                                               // process the observation
	    new_state(?,?,0->(nwindow-1),j) = state(?,?,1->nwindow,j);              // shift the image stack and add a new image
	    new_state(?,?,nwindow-1,j) = img;         
	    saved_games(?,?,igame,i) = img;
	    saved_actions(igame,i) = actions(i);
	    saved_preds(?,igame,i) = preds(?,i).reshapeView(nactions,1,1);
	}    
	total_epochs += sum(dones).v.toInt;
	block_reward += sum(rewards).v;

	times(3) = toc;
    
	dones <-- (dones + (rewards != 0f) > 0f);

	if (sum(dones).v > 0) rbaseline = baseline_decay * rbaseline + (1-baseline_decay) * (sum(rewards).v / sum(dones).v);
	if (! dobaseline && rbaseline - rbaseline0 > baselinethresh * (score_range(1) - score_range(0))) {
	    dobaseline = true;
	    rbaseline0 = rbaseline;
	}
	val arewards = if (dobaseline) {
	    rewards - (dones > 0) *@ (rbaseline - rbaseline0);
	} else {
	    rewards;
	}
	state_memory(?,?,?,(i*npar)->((i+1)*npar)) = state;
	action_memory(i,?) = actions;
	reward_memory(i,?) = arewards;
	done_memory(i,?) = dones;
	state <-- new_state;
	times(4) = toc;
    	dtimes(0,0->4) = dtimes(0,0->4) + (times(0,1->5) - times(0,0->4))
    }
    val (q_values_next, _, _, _, _) = t_estimator.predict(new_state); 
    times(5) = toc;
    
    val v_next = maxi(q_values_next);                        // predict the V values of the next states
    reward_memory(ndqn-1,?) = done_memory(ndqn-1,?) *@ reward_memory(ndqn-1,?) + (1f-done_memory(ndqn-1,?)) *@ v_next; // Add to reward mem if no actual reward
    for (i <- (ndqn-2) to 0 by -1) {
	// Propagate rewards back in time. Actual rewards override predicted rewards. 
	reward_memory(i,?) = done_memory(i,?) *@ reward_memory(i,?) + (1f - done_memory(i,?)) *@ reward_memory(i+1,?) *@ discount_factor;
    }

    // Now compute gradients for the states/actions/rewards saved in the table.
    for (i <- 0 until ndqn) {
	new_state <-- state_memory(?,?,?,(i*npar)->((i+1)*npar))
	val (lv, vv, wv, ev) = q_estimator.gradient(new_state, action_memory(i,?), reward_memory(i,?), npar);  
	block_loss += lv;                                // compute q-estimator gradient and return the loss
	block_vloss += vv; 
	block_wloss += wv; 
	block_entropy += ev; 
    }
    times(6) = toc;
     
    q_estimator.msprop(lr);                       // apply the gradient update
    times(7) = toc;
    dtimes(0,4->7) = dtimes(0,4->7) + (times(0,5->8) - times(0,4->7));

    val t = toc;
    if (istep % printsteps0 == 0) {
        total_reward += block_reward;
        println("I %5d, T %4.1f, L %7.6f, V %7.6f, W %7.6f, Ent %5.4f, E %d, R/E %5.4f, CR/E %5.4f" 
		format(istep, t, block_loss/printsteps/npar, block_vloss/printsteps/npar, block_wloss/printsteps/npar, block_entropy/printsteps/npar, 
		       total_epochs, block_reward/math.max(1,total_epochs-last_epochs), total_reward/math.max(1,total_epochs)));
	reward_plot(istep/printsteps-1) = block_reward/math.max(1,total_epochs-last_epochs);
        last_epochs = total_epochs;
        block_reward = 0f;
        block_loss = 0f;
        block_vloss = 0f;
        block_wloss = 0f;
        block_entropy = 0f;
    }
    //    Mat.debugMem = true;
    //    Mat.debugMemThreshold = 0;
}
Mat.useGPUcache=false
:silent
dtimes
